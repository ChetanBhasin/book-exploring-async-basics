<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Investigating Async Basics - Implementing the Node.js Eventloop in Rust</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body class="light">
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            document.body.className = theme;
            document.querySelector('html').className = theme + ' js';
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="affix"><a href="introduction.html">Introduction</a></li><li><a href="1_concurrent_vs_parallel.html"><strong aria-hidden="true">1.</strong> Concurrent vs Parallel</a></li><li><a href="2_async_history.html"><strong aria-hidden="true">2.</strong> Async history</a></li><li><a href="3_0_the_operating_system.html"><strong aria-hidden="true">3.</strong> The Operating System and CPU</a></li><li><ol class="section"><li><a href="3_1_communicating_with_the_os.html"><strong aria-hidden="true">3.1.</strong> Communicating with the OS</a></li><li><a href="3_1_1_cross_platform_abstractions.html"><strong aria-hidden="true">3.2.</strong> Writing Cross Platform Abstractions</a></li><li><a href="3_2_the_cpu_and_the_os.html"><strong aria-hidden="true">3.3.</strong> The CPU and the OS</a></li></ol></li><li><a href="4_interrupts_firmware_io.html"><strong aria-hidden="true">4.</strong> Interrupts, Firmware and I/O</a></li><li><a href="5_strategies_for_handling_io.html"><strong aria-hidden="true">5.</strong> Strategies for handling I/O</a></li><li><a href="6_0_implementing_the_node_eventloop.html"><strong aria-hidden="true">6.</strong> Implementing the Node Eventloop</a></li><li><ol class="section"><li><a href="6_1_what_is_node.html"><strong aria-hidden="true">6.1.</strong> What is Node?</a></li><li><a href="6_2_node_whats_our_plan.html"><strong aria-hidden="true">6.2.</strong> What's our plan</a></li><li><a href="6_3_node_the_main_loop.html"><strong aria-hidden="true">6.3.</strong> The main loop</a></li><li><a href="6_4_implementing_the_runtime.html"><strong aria-hidden="true">6.4.</strong> Implementing the Runtime</a></li><li><a href="6_5_node_the_threadpool.html"><strong aria-hidden="true">6.5.</strong> The threadpool</a></li><li><a href="6_6_node_the_io_eventqueue.html"><strong aria-hidden="true">6.6.</strong> The I/O eventqueue</a></li><li><a href="6_7_shortcuts_and_improvements.html"><strong aria-hidden="true">6.7.</strong> Shortcuts and improvements</a></li><li><a href="6_8_node_final_code.html"><strong aria-hidden="true">6.8.</strong> Final code</a></li></ol></li><li><a href="conclusion.html">Conclusion</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar" class="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div class="left-buttons">
                            <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                                <i class="fa fa-bars"></i>
                            </button>
                            <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                                <i class="fa fa-paint-brush"></i>
                            </button>
                            <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                                <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                                <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                            </ul>
                            
                            <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                                <i class="fa fa-search"></i>
                            </button>
                            
                        </div>

                        <h1 class="menu-title">Investigating Async Basics - Implementing the Node.js Eventloop in Rust</h1>

                        <div class="right-buttons">
                            <a href="print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button" class="fa fa-print"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>This book aims to take a deep down into the why and how of concurrent programming. First we build
a good foundation of knowledge, before we use that knowledge to implement a toy version of the
Node.js runtime.</p>
<blockquote>
<p>This book is developed in the open and has <a href="https://github.com/cfsamson/book-investigating-async-basics">it's repository here</a>.
The final code in this book is located here if you want to clone it and play with it or improve it.</p>
</blockquote>
<p>Don't block the eventloop! Don't poll in a loop! Increase throughput! Concurrency.
Parallelism.</p>
<p>You've most likely heard and read about this many times before,
and maybe, at some point you've thought you understood everything
only to find yourself confused a moment later. Especially when you want to
understand how it works on three different Operating Systems.</p>
<p>Me too.</p>
<p>So I spent a couple of hundred hours to try to fix that for myself. Then I wrote
this story and now I invite you to join me on that journey.</p>
<p>I warn you though, we need to venture from philosophical heights where we try to
formally define a &quot;task&quot; all the way down to the deep waters where firmware and
other strange creatures rule (I believe some of the more wicked creatures there
are tasked with naming low level OS syscalls and structures on Windows. However, I
have yet to confirm this.).</p>
<blockquote>
<p>Everything in this book will cover the topics for the three major Operating Systems
Linux, Macos and Windows.</p>
</blockquote>
<h2><a class="header" href="#who-is-this-book-for" id="who-is-this-book-for">Who is this book for?</a></h2>
<p>You'll have to be adventurous, curious, and a bit forgiving. Even though we
cover some complex topics we'll have to simplify them significantly to be able
to learn anything from them in a small book. You can probably spend the better
part of a career becoming an expert in several of the fields we cover, so forgive
me already now for not being able to cover all of them with the precision,
thoroughness and respect they deserve.</p>
<p>However, this book might be interesting for you if you:</p>
<ul>
<li>
<p>Want to know the difference between parallel and concurrent, and finding a mental model to explain why concurrency is valuable.</p>
</li>
<li>
<p>Are curious on how to make syscalls on three different platforms using only Rusts standard library.</p>
</li>
<li>
<p>Think it's fun to see if you can generate a software interrupt and make a syscall using inline assembly.</p>
</li>
<li>
<p>Want to know more about how OS and the CPU handles concurrency.</p>
</li>
<li>
<p>Think figuring out how your code, the OS, your CPU, a device driver and some firmware handles I/O is interesting.</p>
</li>
<li>
<p>Can accept a detour where we see how the CPU &quot;knows&quot; if a memory address is invalid.</p>
</li>
<li>
<p>Have read enough articles about it but want to know more about what the Node.js eventloop really is, and why most diagrams of it on the web are pretty misleading.</p>
</li>
<li>
<p>Think using the knowledge from our research to write a <strong>toy</strong> node.js runtime is pretty cool.</p>
</li>
<li>
<p>Already know some Rust but want to learn more.</p>
</li>
</ul>
<p>So, what du you think? Is the answer yes? We'll then join me on this venture
where we try to get a better understanding of all these subjects.</p>
<blockquote>
<p>We'll only use Rusts standard library. The reason for this is that we really want to know how tings
work, and Rusts standard library strikes the perfect balance for this task providing abstractions
but they're thin enough to let us easily peek under the covers to see how things work.</p>
</blockquote>
<p>You don't have to be a Rust programmer to follow along. This book will have numerous chapters where
we explore concepts, and where the code examples are small and easy to understand, but it will
be more code towards the end and you'll get the most out of it by learning the basics first.</p>
<p>I do recommend that you read my book preceding this <a href="https://app.gitbook.com/@cfsamson/s/green-threads-explained-in-200-lines-of-rust/">Green threads explained in 200 lines of Rust</a>
since I cover quite a bit about Rust, stacks, threads and inline assembly there and
will not repeat everything here. However, it's definitely not a must.</p>
<blockquote>
<p><a href="https://www.rust-lang.org/tools/install">You will find everything you need to set up Rust here</a></p>
</blockquote>
<h2><a class="header" href="#following-along" id="following-along">Following along</a></h2>
<p>For this book I use <code>mdbook</code>, which has the nice benefit of being able to run
the code we write directly in the book. However, this only runs the Linux version
and since part of the challenge here is to write for three platforms I've included
and explained that code too, but you'll have to copy it over and run it yourself.</p>
<h2><a class="header" href="#disclaimer" id="disclaimer">Disclaimer</a></h2>
<ol>
<li>
<p>We'll implement a <strong>toy</strong> version of the Node.js eventloop (a bad, but working and conceptually similar eventloop)</p>
</li>
<li>
<p>We'll <strong>not</strong> primarily focus on code quality and safety, though this is important,
I will focus on understanding the concepts and ideas behind the code. We will have to make
many shortcuts to keep this concise and short.</p>
</li>
<li>
<p>I will however do my best to point out hazards and the shortcuts we make.
I will try to point out obvious places we could do a better job or take big
shortcuts.</p>
</li>
</ol>
<p>If you see something that is imprecise or even wrong I really hope you'll consider
contributing to make this better for the next person reading it. </p>
<h2><a class="header" href="#credits" id="credits">Credits</a></h2>
<p>Substantial contributions will be credited here.</p>
<h2><a class="header" href="#why-i-wrote-this-and-companion-books" id="why-i-wrote-this-and-companion-books">Why I wrote this and companion books</a></h2>
<p>This started as a wish to write an article about Rusts Futures 3.0, but has now
expanded into 3 finished books about concurrency in general and hopefully, at
some point a fourth about Rusts Futures exclusively.</p>
<p>This process has also made me realize why I have vague memories from my childhood
of threats being made about stopping the car and letting me off if I didn't stop
asking &quot;why?&quot; to everything.</p>
<p>Basically, the list below is a result of this urge to understand <em>why</em> while
reading the RFC's and discussions about Rusts async story: </p>
<ul>
<li><a href="https://app.gitbook.com/@cfsamson/s/green-threads-explained-in-200-lines-of-rust/">Green threads explained in 200 lines of Rust</a></li>
</ul>
<p>A book where we explore green threads by implementing our own green threads in Rust.</p>
<ul>
<li>Investigating Async Basics by Implementing the Node.js Eventloop in Rust</li>
</ul>
<p>This is the book you're reading now.</p>
<ul>
<li>Investigating Epoll, Kqueue and IOCP with Rust</li>
</ul>
<p>Will be released October 2. 2019. We implement an extremely simple and limited
cross platform eventloop based on Epoll, Kqueue and IOCP. Even though it's simple
and bad, it will be working and will be &quot;easy&quot; to understand. A good place
to start if you want to dig further.</p>
<p>We use this library in this book, but it was too much to include here.</p>
<ul>
<li>Investigating Rusts Futures (TBD)</li>
</ul>
<p>This book has not even started yet, and I will see if I can provide a useful
alternative or add anything usefull at all since there is so much being written
about this right now.</p>
<h1><a class="header" href="#whats-the-difference-between-concurrency-and-parallelism" id="whats-the-difference-between-concurrency-and-parallelism">What's the difference between concurrency and parallelism?</a></h1>
<p>We’ll right of the bat dive into this subject by defining what concurrency is, and since it’s very easy to confuse concurrent with parallel we have to try to make the difference clear from the get go.</p>
<blockquote>
<p>Concurrency is about <strong>dealing</strong> with a lot of things at the same time.</p>
</blockquote>
<blockquote>
<p>Parallelism is about <strong>doing</strong> a lot of things at the same time.</p>
</blockquote>
<p>We call the concept of progressing multiple tasks at the same time <code>Multitasking</code>.
There are several ways to multitask. One is by <strong>progressing</strong> tasks concurrently,
but not at the same time. Another is to progress two tasks at the same time in parallel.
Parallelism can a way of achieving of concurrency.</p>
<p>When we progress tasks concurrently we say they progress in an asynchronous manner.
Asynchronous code execution is therefore the way we handle concurrency in programming.</p>
<p><img src="./images/definitions.jpg" alt="paralell vs concurrency" /></p>
<h2><a class="header" href="#lets-start-off-with-some-definitions" id="lets-start-off-with-some-definitions">Lets start off with some definitions</a></h2>
<h3><a class="header" href="#resource" id="resource">Resource</a></h3>
<p>Something we need to be able to progress a task. Our resources is limited. This
could be CPU time or memory.</p>
<h3><a class="header" href="#task" id="task">Task</a></h3>
<p>A set of operations that requires some kind of resource to progress. A task must
consist of several sub-operations.</p>
<h3><a class="header" href="#parallel" id="parallel">Parallel</a></h3>
<p>Something happening independently at the <strong>exact</strong> same time.</p>
<h3><a class="header" href="#concurrent" id="concurrent">Concurrent</a></h3>
<p>Tasks that are <strong><code>in progress</code></strong> at the same time, but not <em>necessarily</em> progressing
simultaneously. </p>
<p>This is an important distinction. If two tasks are running concurrently, 
but are not running in parallel, they must be able to stop and resume their progress. 
We say that a task is <code>interruptable</code> if it allows for this kind of concurrency.</p>
<h2><a class="header" href="#the-mental-model-i-use" id="the-mental-model-i-use">The mental model I use.</a></h2>
<p>I firmly believe the main reason we find parallel and concurrent programming 
hard to reason about stems from how we model events in our everyday life. We 
tend to define these terms loosely so our intuition is often wrong. </p>
<blockquote>
<p>It doesn't help that <strong>concurrent</strong> is defined in the dictionary as: <em>operating or occurring at the same time</em> which 
doesn't really help us much when trying to describe how it differs from <strong>parallel</strong></p>
</blockquote>
<p>For me, this first clicked when I started to understand why we want to make a distinction between parallel and concurrent in the first place!</p>
<p>The <strong>why</strong> has everything to do with resource utilization and <a href="https://en.wikipedia.org/wiki/Efficiency">efficiency</a>.</p>
<blockquote>
<p>Efficiency is the (often measurable) ability to avoid wasting materials, energy, efforts, money, and time in doing something or in producing a desired result.</p>
</blockquote>
<h3><a class="header" href="#parallelism" id="parallelism">Parallelism</a></h3>
<p>Is increasing the resources we use to solve a task. It has nothing to do with <em>efficiency</em>.</p>
<h3><a class="header" href="#concurrency" id="concurrency">Concurrency</a></h3>
<p>Has everything to do with efficiency and resource utilization. Concurrency can never make <em>one single task go faster</em>. 
It can only help us utilize our resources better and thereby <em>finish a set of tasks faster</em>.</p>
<h3><a class="header" href="#lets-draw-some-parallels-to-process-economics" id="lets-draw-some-parallels-to-process-economics">Let's draw some parallels to process economics</a></h3>
<p>In businesses that manufacture goods, we often talk about LEAN processes. And 
this is pretty easy to compare with why programmers care so much about what we can 
achieve if we handle tasks concurrently. </p>
<p>I'll let let this 3 minute video explain it for me:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Oz8BR5Lflzg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>Ok, so it's not the newest video on the subject, but it's explains a lot in 3 minutes. Most importantly the gains we try to achieve when applying LEAN techniques, and most importantly: <strong>eliminate waiting and non-value-adding tasks.</strong></p>
<blockquote>
<p>In programming we could say that we want to avoid <code>blocking</code> and <code>polling</code> (in a busy loop).</p>
</blockquote>
<p>Now would adding more resources (more workers) help in the video above? Yes, but we use double the resources to produce the same output as one person with an optimal process could do. That's not the best utilization of our resources.</p>
<blockquote>
<p>To continue the parallel we started, we could say that we could solve the problem of a freezing UI while waiting for an I/O event to occur 
by spawning a new thread and <code>poll</code>in a loop or <code>block</code> there instead of our main thread. However, that new 
thread is either consuming resources doing nothing, or worse, using one core to busy loop while checking if 
an event is ready. Either way it's not optimal, especially if you run a server you want to utilize fully.</p>
</blockquote>
<p>If you consider the coffee machine as some I/O resource, we would like to start that process, then move on to preparing the 
next job, or do other work that needs to be done instead of waiting.</p>
<p><em>But that means there are things happening in parallel here?</em></p>
<p>Yes, the coffee machine is doing work while the &quot;worker&quot; is doing
maintenance and filling water. But this is the crux: <em>Our reference frame is the worker, not the whole system. The guy making coffee is your code.</em> </p>
<blockquote>
<p>It's the same when you make a database query. After you've sent the query to the database server, 
the CPU on the database server will be working on your request while you wait for a response. In &gt; practice, it's a way
of parallelizing your work. </p>
</blockquote>
<p><strong>Concurrency is about working smarter. Parallelism is a way of throwing more resources at the problem.</strong></p>
<h2><a class="header" href="#concurrency-and-its-relation-to-io" id="concurrency-and-its-relation-to-io">Concurrency and its relation to I/O</a></h2>
<p>As you might understand from what I've written so far, writing async code mostly
makes sense when you need to be smart to make optimal use of your resources.</p>
<p>Now, if you write a program that is working hard to solve a problem, there often is no help
in concurrency, this is where parallelism comes in to play since it gives you
a way to throw more resources at the problem if you can split it into parts that
you can work on in parallel.</p>
<p><strong>I can see two major use cases for concurrency:</strong></p>
<ol>
<li>When performing I/O and you need to wait for some external event to occur</li>
<li>When you need to divide your attention and prevent one task from waiting too 
long</li>
</ol>
<p>The first is the classic I/O example, where you will have to wait for a network
call, a file operation or something else to happen before you can progress one 
task, but you have many tasks to do so instead of waiting you continue work 
elsewhere and either check in regularly to see if the task is ready to progress
or make sure you are notified when that task is ready to progress.</p>
<p>The second is an example that is often the case when having an UI. Let's pretend
you only have one core. How do you prevent the whole UI from becoming unresponsive
while performing other CPU intensive tasks?</p>
<p>Well, you can stop what ever task you're doing every 16ms, and run the &quot;update UI&quot;
task, and then resume whatever you were doing afterwards. This way, you will have
to stop/resume your task 60 times a second, but you will also have a fully 
responsive UI which has roughly a 60 Hz refresh rate.</p>
<h2><a class="header" href="#what-about-threads-provided-by-the-os" id="what-about-threads-provided-by-the-os">What about threads provided by the OS?</a></h2>
<p>We'll cover threads a bit more when we talk about operating systems, but I'll 
mention them here as well. The problem with threads provided by the operating 
system is that they appear to be mapped to cores. But that is not necessarily 
the truth even though most operating systems will try to map one thread to a 
core up to the number of threads is equal to  the number of cores.</p>
<p>Once we create more threads than there are cores, the OS will switch between our
threads and progress each of them <code>concurrently</code> using the scheduler to give each
thread some time to run. So in this way, threads can be a means to perform tasks
in parallel, but they can also be a means to achieve concurrency.</p>
<p>This brings me over to the last part about concurrency. It needs to be defined
in some sort of reference frame.</p>
<h2><a class="header" href="#changing-the-reference-frame" id="changing-the-reference-frame">Changing the reference frame</a></h2>
<p>When you write code that is perfectly synchronous from your perspective, let's take a look at how that looks from the operating system perspective.</p>
<p>The Operating System might not run your code from start to end at all. It might stop and resume your process many times. The CPU might get interrupted and handle som inputs while you think it's only focused on your task. </p>
<p>So synchronous execution is only an illusion. But from the perspective of you as a programmer it's not, and that is the important takeaway:</p>
<p>When we talk about concurrency without providing any other context we are using you as a programmer and your code (your process) as the reference frame. If you start pondering about concurrency
whithout keeping this in the back of your head it will get confusing very fast.</p>
<p>The reason I spend so much time on this is that once you realize that, you'll start to see that some of the things you hear and learn that might seem contradicting really is not. You'll just have to consider the reference frame first.</p>
<p>If this still sounds complicated, I understand. Just sitting and reflecting about concurrency is difficult, but if we try to keep these thoughts in the back of our head when we work with code I promise it will get less and less confusing.</p>
<h1><a class="header" href="#async-history" id="async-history">Async history</a></h1>
<p>In the start computers had one CPU and it executed a set of instructions written
by a programmer one by one. No scheduling, no threads, no multitasking. This was
how computers worked for a long time. We're talking back when the days where a
program looked like this:</p>
<p><img src="./images/punched_card_deck.jpg" alt="Image" /></p>
<p>There were operating systems being researched though and when personal computing
started to grow in the 80's we had operating systems like DOS. They usually
yielded control of the entire CPU to the program currently executing and it was
up to the programmer to make things work and implement any kind of multitasking
for their program. This worked fine, but as interactive UIs using a mouse and
windowed operating systems became the norm, this simply couldn't work anymore.</p>
<h2><a class="header" href="#non-preemptive-multitasking" id="non-preemptive-multitasking">Non-preemptive multitasking</a></h2>
<p>The method used to be able to keep the UI interactive and running background
processes, was accomplished by non-preemtive multitasking. This kind of 
multitasking put the responsibility of letting the OS run other tasks like 
responding to input from the mouse, or running a background task in the hands of
the programmer. Typically the programmer <code>yielded</code> control to the OS.</p>
<p>Beside off loading a huge responsibility to every programmer writing a program
for your platform, this was also error prone. A small mistake in a programs code
could halt or crash the entire system. If you remember Windows 95, you also
remember the times when a window hung and you could paint the entire screen with
it (almost the same way as the end in Solitare, the card game that came with Windows).
This was reportedly a typical error in the code that was supposed to yield control
to the operating system.</p>
<p>If you're not sure about what this kind of threaded multitasking is I wrote about
recommend my previous book that explains this part of multitasking pretty well.
You'll know everything you need about threads, contexts, stacks and scheduling
for following along.</p>
<h2><a class="header" href="#preemtive-multitasking" id="preemtive-multitasking">Preemtive multitasking</a></h2>
<p>While non-preemtive multitasking sounded like a good idea, it turned out to
create serious problems as well. I will not list them here but as you can imagine,
letting every program and programmer out there be responsible for parts of the
scheduling of tasks in an operating system will be chaos and ultimately lead to
a bad user experience.</p>
<p>The solution was to place the responsibility of scheduling the CPU resources
between the programs that requested it (including to OS itself) in the hands of
the OS. The OS can stop execution of a process, do something else, and switch back.</p>
<p>In a single core machine you can visualize this as running a program you wrote,
and the OS stops to update the mouse position, and switches back to your program.
This can happen many times each second, not only to keep the UI responsive but
it can also give some time to other background tasks and IO events.</p>
<p>This is now the prevailing way to design an operating system.</p>
<h2><a class="header" href="#hyperthreading" id="hyperthreading">Hyperthreading</a></h2>
<p>As CPU's evolved and added more functionality like several ALUs (Algorithmic Logical Unit) 
and more logical units in general, the CPU manufacturers realized that the entire
CPU was never utilitized fully. For example when an operation only required some
parts of the CPU, an instruction could be run on the ALU simultaneously. This
became the start of Hyperthreading.</p>
<p>You see, on your computer today that it has i.e. 6 cores, and 12 logical cores.
This is exactly where Hyperthreading comes in. It &quot;simulates&quot; two cores on the
same core by using unused parts of the CPU to drive progress on thread &quot;2&quot;
simultaneously as it's running the code on thread &quot;1&quot;. It does this by using a
number of smart tricks (like the one with the ALU).</p>
<p>Now we could actually offload some work on one thread while keeping the UI
interactive by responding to events in the second thread even though we only
had one CPU core.</p>
<blockquote>
<p>You might wonder about the performance of Hyper Threading? </p>
<p>It turns out that Hyperthreading has been continuously improved since the 90's.
Since you're not actually running two CPU's there will be some operations that
need to wait for each other to finish. The performance gain of hyperthreading
compared to multithreading in a single core seems to be <a href="https://en.wikipedia.org/wiki/Hyper-threading#Performance_claims">somewhere close
to 30 %</a> but
it largely depends in the workload.</p>
</blockquote>
<h2><a class="header" href="#multicore-processors" id="multicore-processors">Multicore processors</a></h2>
<p>As most know, the clock frequency of processors has been flat for a long time.
Processors get faster by improving caches, branch prediction, speculative execution
and working on the processing pipelines of the processors, but the gains seems to
be diminishing.</p>
<p>On the other hand, new processors are so small they allow us to have many on the
same chip instead. Now most CPUs have many cores, including hyperthreading.</p>
<h2><a class="header" href="#so-how-synchronous-is-the-code-you-write-really-" id="so-how-synchronous-is-the-code-you-write-really-">So how synchronous is the code you write, really ?</a></h2>
<p>As many things this depends on your perspective. From the perspective of your process
and the code you write for it, everything will normally happen in the order you
write it.</p>
<p>From the OS perspective it might, or might not, interrupt your code, pause it
and run some other code in the meantime before resuming your process.</p>
<p>From the perspective of the CPU it will mostly execute instructions one at a time[1].
They don't care who wrote the code though so when a hardware interrupt happens,
they will immediately stop and give control to an interrupt handler. This is how
the CPU handles concurrency.</p>
<blockquote>
<p>[1] However, modern CPU can also do a lot if things in parallel. Most CPUs are
pipelined, meaning that the next instruction is loaded while the current is
executing. It might have a branch predictor that tries to figure out what
instructions to load next. The processor can also reorder instructions by using
&quot;out of order execution&quot; if it believes it makes things faster this way without
&quot;asking&quot; or &quot;telling&quot; the programmer or the OS so you might not have any guarantee
that A happens before B. The CPU offloads some work to separate &quot;coprocessors&quot;
like the FPU for for floating point calculations leaving the main CPU ready to
do other tasks et cetera.</p>
<p>As a high level overview, it's OK to model the CPU as operating in a synchronous
manner, but lets for now just make a mental note that this is a model with some
caveats that becomes especially important when talking about parallelism,
synchronization primitives like mutexes and atomics and security.</p>
</blockquote>
<h1><a class="header" href="#the-operating-system" id="the-operating-system">The Operating System</a></h1>
<p>The operating system stands in the center of everything we do as programmers,
so there is no way for us to discuss any kind of fundamentals in programming
without talking about operating systems in a bit of detail. </p>
<h2><a class="header" href="#concurrency-from-the-operating-systems-perspective" id="concurrency-from-the-operating-systems-perspective">Concurrency from the operating systems perspective</a></h2>
<blockquote>
<p>Operating systems has been &quot;faking&quot; synchronous execution since the 90's.</p>
</blockquote>
<p>This ties into what I talked about in the first chapter when I said that <code>concurrent</code>
needs to be talked about within a reference frame and I explained that the OS
might stop and start your process at any time.</p>
<p>What we call synchronous code is in most cases code that appears as synchronous 
to us as programmers. Neither the OS or the CPU live in a fully synchronous world.</p>
<p>Operating systems uses <code>preemptive multitasking</code> and as long as the operating 
system you're running is preemptively scheduling processes, you won't have a 
guarantee that your code runs instruction by instruction without interruption. </p>
<p>The operating system will make sure that all important processes gets some time from the CPU to make progress.</p>
<blockquote>
<p>This is not as simple when we're talking about modern machines with 4-6-8-12
physical cores since you might actually execute code on one of the CPU's
uninterrupted if the system is under very little load. The important part here
is that you can't know for sure and there is no guarantee that you code will be
left to run uninterrupted.</p>
</blockquote>
<h2><a class="header" href="#teaming-up-with-the-os" id="teaming-up-with-the-os">Teaming up with the OS.</a></h2>
<p>When programming it's often easy to forget how many moving pieces that need to
cooperate to reach maximum efficiency. When you make a web request, you're not
asking the CPU or the network card to do something for you, you're asking the
operating system to talk to the network card for you.</p>
<p>There is no way for you as a programmer to make your system optimally efficient
without playing to the operating systems strengths. You basically don't have
access to the hardware directly. This assumes that you're writing code that runs
on an operating system though. But our focus here is written in the context of
Linux, Macos and Windows so cooperating with the OS to be as efficient as possible
is unavoidable.</p>
<p>However this also means that to understand everything from the ground up, you'll
also need to know how your operating system handles these tasks.</p>
<p>To be able to work with the operating system, we'll need to know how we can communicate with it and that's exactly what we're going to go through next.</p>
<h1><a class="header" href="#communicating-with-the-operating-system" id="communicating-with-the-operating-system">Communicating with the operating system</a></h1>
<p>In this chapter I want to dive into:</p>
<ul>
<li>What is a System Call</li>
<li>Abstractions over syscalls</li>
<li>Challenges low level cross platform code</li>
</ul>
<h2><a class="header" href="#what-is-a-syscall" id="what-is-a-syscall">What is a syscall</a></h2>
<p>Communication with the operating system is done through <code>System Calls</code> or 
&quot;syscalls&quot;. This is a public API that the operating system provides and that programs
can use to communicate with the OS. Most of the time these calls are abstracted away for us as 
programmers by the language or the runtime we use. A language like Rust makes it 
trivial to make a syscall though which we'll see below.</p>
<p>Now syscalls is an example of something that is unique to the kernel you're communicating with, but the UNIX family of kernels has many similarities especially the API exposed by <code>libc</code>.</p>
<p>Windows on the other hand  uses it's own api, often referred to as WinAPI, and that can be radically different from how the UNIX based systems operate. Most often though there is a way to achieve the same things. In terms of functionality you might not notice a big difference but as we'll see below and especially when we dig into how <code>epoll</code>, <code>kqueue</code> and <code>IOCP</code>, they can differ a lot on how this functionality is implemented.</p>
<h2><a class="header" href="#syscall-example" id="syscall-example">Syscall example</a></h2>
<p>To get a bit more familiar with syscalls we'll implement a very basic one for 
the three arcitectures: BSD(macos), Linux and Windows. We'll also see how this is implemented in three levels of abstractions.</p>
<p>The syscall we'll implement is the one used when we write something to <code>stdout</code> since that is such a common operation it's interesting to se how it really works.</p>
<h3><a class="header" href="#the-lowest-level-of-abstraction" id="the-lowest-level-of-abstraction">The lowest level of abstraction</a></h3>
<p>For this to work we need to write some inline assembly again. I'll skip the main function since that should be easy to understand now, and focus on the instructions we write to the CPU.</p>
<blockquote>
<p>If you want a more torough introduction to inline assembly I can refer you to the <a href="https://cfsamson.gitbook.io/green-threads-explained-in-200-lines-of-rust/an-example-we-can-build-upon">relevant chapter in my previous book</a> if you haven't read it already.</p>
</blockquote>
<p>Now at this level of abstraction, we'll write different code for all three platforms.</p>
<p>On both Linux and Macos the syscall we want to invole is called <code>write</code>. Both systems operates based on the concept of <code>file descriptors</code> and <code>stdout</code> is one of these alredy present when you start a proccess.</p>
<p><strong>On Linux a <code>write</code> syscall can look like this</strong></p>
<blockquote>
<p>If you want a more torough introduction to inline assembly I can refer you to the <a href="https://cfsamson.gitbook.io/green-threads-explained-in-200-lines-of-rust/an-example-we-can-build-upon">relevant chapter in my previous book</a> if you haven't read it already.</p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust">#![feature(asm)]
fn main() {
    let message = String::from(&quot;Hello world from interrupt!\n&quot;);
    syscall(message);
}

#[cfg(target_os = &quot;linux&quot;)]
fn syscall(message: String) {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    unsafe {
        asm!(&quot;
        mov     $$1, %rax   # system call 1 is write on linux
        mov     $$1, %rdi   # file handle 1 is stdout
        mov     $0, %rsi    # address of string to output
        mov     $1, %rdx    # number of bytes
        syscall             # call kernel, syscall interrupt
    &quot;
        :
        : &quot;r&quot;(msg_ptr), &quot;r&quot;(len)

        )
    }
}
</code></pre></pre>
<p>The code to initiate the <code>write</code> syscall on Linux is <code>1</code> so when we write <code>$$1</code> we're writing the literal value 1 to the <code>rax</code> register.</p>
<blockquote>
<p><code>$$</code> in inline assembly using the AT&amp;T syntax is how you write a literal value. A single <code>$</code> means you're referring to parameter so when we write <code>$0</code> we're referring to the first parameter <code>msg_ptr</code>.</p>
</blockquote>
<p>Coincidentally, placing the value <code>1</code> in to the <code>rdi</code> register means that we're referring to <code>stdout</code> which is the file descriptor we want to write to. This has nothing to do with the fact that the <code>write</code> syscall also has the code <code>1</code>.</p>
<p>Secondly we pass in the address of our string buffer and the length of the buffer in the registers <code>rsi</code> and <code>rdx</code> respectively, and call the <code>syscall</code> instruction.</p>
<blockquote>
<p>The <code>syscall</code> is a rather new one. On the earlier 32-bit systems in the <code>x86</code> arcitecture, you invoked a syscall by issuing a software interrupt <code>int 0x80</code>. A software interrupt is considered slow at the level we're working at here so later it was added a seperate instruction for it called <code>syscall</code>. he <code>syscall</code> instruction uses <a href="http://articles.manugarg.com/systemcallinlinux2_6.html">VDSO</a> which is a memory page attached to each process so no context switch is necessary to execute the system call.</p>
</blockquote>
<p>On Macos, the syscall will look something like this:</p>
<pre><pre class="playpen"><code class="language-rust">#![feature(asm)]
fn main() {
    let message = String::from(&quot;Hello world from interrupt!\n&quot;);
    syscall(message);
}

#[cfg(target_os = &quot;macos&quot;)]
fn syscall(message: String) {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    unsafe {
        asm!(
            &quot;
        mov     $$0x2000004, %rax   # system call 0x2000004 is write on macos
        mov     $$1, %rdi           # file handle 1 is stdout
        mov     $0, %rsi            # address of string to output
        mov     $1, %rdx            # number of bytes
        syscall                     # call kernel, syscall interrupt
    &quot;
        :
        : &quot;r&quot;(msg_ptr), &quot;r&quot;(len)
        )
    };
}
</code></pre></pre>
<p>As you see this is not that different from the one we wrote for Linux, with the exception of the fact that syscall <code>write</code> has the code <code>0x2000004</code> instead of <code>1</code>.</p>
<p><strong>What about Windows?</strong></p>
<p>It's a good opportunity to explain why writing code like we do above is a bad idea. You see, if you want your code to work for a long time you have to worry about what <code>guarantees</code> the OS gives you. As far as I know, there are no guarantees that <code>$$0x2000004</code> on Macos will always refer to <code>write</code>. I do think Linux has better guarantees, but I know for a fact that Windows makes absolutely zero guarantees about this.</p>
<p>Windows has changed it's internals about this numerous times and provide no official documentation. The only thing we got is reverse engineered tables like <a href="https://j00ru.vexillium.org/syscalls/nt/64/">this</a>. That means that what was <code>write</code> can be changed to <code>delete</code> the next time you update Windows.</p>
<p>Even though it would be fun, as a curiosity, to include it I haven't managed to get it to work and have no idea how much code that would be. See <a href="./introduction.html">contributing</a> if you have the answer dear reader, and I'll include it if it's not too much code.</p>
<h2><a class="header" href="#the-next-level-of-abstraction" id="the-next-level-of-abstraction">The next level of abstraction</a></h2>
<p>The next level of abstraction is to use the API which all three operating systems provide for us. Already we can see that this abstraction helps us remove some code since fortunately for us in this specific example, the syscall is the same on Linux and on Macos so we only need to worry if we're on Windows and therefore use the <code>#[cfg(not(target_os = &quot;windows&quot;))]</code> conditional compilation flag. For the Windows syscall we do the opposite.</p>
<h3><a class="header" href="#using-the-api-in-linux-and-macos" id="using-the-api-in-linux-and-macos">Using the API in Linux and Macos</a></h3>
<p>You can run this code directly here in the window. However, the Rust playground 
runs on Linux, you'll need to copy the code over to a Windows machine if you 
want to try it out the code for Windows further down.</p>
<pre><pre class="playpen"><code class="language-rust">use std::io;

fn main() {
    let sys_message = String::from(&quot;Hello world from syscall!\n&quot;);
    syscall(sys_message).unwrap();
}

// and: http://man7.org/linux/man-pages/man2/write.2.html
#[cfg(not(target_os = &quot;windows&quot;))]
#[link(name = &quot;c&quot;)]
extern &quot;C&quot; {
    fn write(fd: u32, buf: *const u8, count: usize) -&gt; i32;
}

#[cfg(not(target_os = &quot;windows&quot;))]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    let res = unsafe { write(1, msg_ptr, len) };

    if res == -1 {
        return Err(io::Error::last_os_error());
    }
    Ok(())
}

</code></pre></pre>
<p>I'll explain what we just did here. I assume that the <code>main</code> method needs no 
comment.</p>
<pre><code class="language-rust no_run noplaypen">#[link(name = &quot;c&quot;)]
</code></pre>
<p>Every Linux installation comes with a version of <code>libc</code> which a C-library for 
communicating with the operating system. Having a <code>libc</code> with a consistent API 
means they can change the underlying implementation without braking everyones 
code. This flag tells the compiler to link to the &quot;c&quot; library on the system we'
re compiling for.</p>
<pre><code class="language-rust no_run noplaypen">extern &quot;C&quot; {
    fn write(fd: u32, buf: *const u8, count: usize);
}
</code></pre>
<p><code>extern &quot;C&quot;</code> or only <code>extern</code> (C is assumed if nothing is specified) means we're 
linking to specific functions in the &quot;c&quot; library using the &quot;C&quot; calling 
convention. As you'll see on Windows we'll need to change this since it uses a 
different calling convention than the UNIX family.</p>
<p>The function we're linking to needs to have the exact same name, in this case 
<code>write</code>. The parameters doesn't need to have the same name but they must be in 
the right order and it's good practice to name them the same as in the library 
you're linking to.</p>
<p>The write function takes a <code>file descriptor</code> which in this case is a handle to 
<code>stdout</code>, a pointer to a array of <code>u8</code> values, and a count of how many values we 
want to read from the buffer.</p>
<pre><code class="language-rust no_run noplaypen">#[cfg(not(target_os = &quot;windows&quot;))]
fn syscall_libc(message: String) {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    unsafe { write(1, msg_ptr, len) };
}
</code></pre>
<p>The first thing we do is to get the pointer to the underlying buffer for our 
string. That will be a pointer of type <code>*const u8</code> which matches our <code>buf</code> 
argument. The length of the message corresponds to the <code>count</code> argument.</p>
<p>You might ask how we know that <code>1</code> is the file handle to <code>stdout</code> and where we 
found that value. You'll notice this a lot when writing syscalls from Rust. 
Usually constants are defined in the C header files which we can't link to, so 
we need to search them up. 1 is always the file descriptor for <code>stdout</code> on UNIX 
systems.</p>
<p>A call to a FFI function is always unsafe so we need to use the <code>unsafe</code> keyword 
here.</p>
<h3><a class="header" href="#using-the-api-on-windows" id="using-the-api-on-windows">Using the API on Windows</a></h3>
<pre><code class="language-rust no_run noplaypen">use std::io;

fn main() {
    let sys_message = String::from(&quot;Hello world from syscall!\n&quot;);
    syscall(sys_message).unwrap();
}

#[cfg(target_os = &quot;windows&quot;)]
#[link(name = &quot;kernel32&quot;)]
extern &quot;stdcall&quot; {
    /// https://docs.microsoft.com/en-us/windows/console/getstdhandle
    fn GetStdHandle(nStdHandle: i32) -&gt; i32;
    /// https://docs.microsoft.com/en-us/windows/console/writeconsole
    fn WriteConsoleW(
        hConsoleOutput: i32,
        lpBuffer: *const u16,
        numberOfCharsToWrite: u32,
        lpNumberOfCharsWritten: *mut u32,
        lpReserved: *const std::ffi::c_void,
    ) -&gt; i32;
}

#[cfg(target_os = &quot;windows&quot;)]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {

    // let's convert our utf-8 to a format windows understands
    let msg: Vec&lt;u16&gt; = message.encode_utf16().collect();
    let msg_ptr = msg.as_ptr();
    let len = msg.len() as u32;
    
    let mut output: u32 = 0;
        let handle = unsafe { GetStdHandle(-11) };
        if handle  == -1 {
            return Err(io::Error::last_os_error())
        }

        let res = unsafe { 
            WriteConsoleW(handle, msg_ptr, len, &amp;mut output, std::ptr::null()) 
            };
        if res  == 0 {
            return Err(io::Error::last_os_error());
        }

    assert_eq!(output as usize, len);
    Ok(())
}
</code></pre>
<pre><code>The Rust playground, which we use to run our code, is a Linux machine. I 
disabled the possibility to run the windows code here (since it will 
essentially just be skipped on compilation). However, if you have a Windows 
machine, copy the code above and try for yourself.
</code></pre>
<p>Now, just by looking at the code above you see it starts to get a bit more 
complex, but let's spend som time to go through line by line what we do here as 
well.</p>
<pre><code class="language-text">#[cfg(target_os = &quot;windows&quot;)]
#[link(name = &quot;kernel32&quot;)]
</code></pre>
<p>The first line is just telling the compiler to only compile this if the 
<code>target_os</code> is Windows.</p>
<p>The second line is a linker directive, telling the linker we want to link to the 
library <code>kernel32</code> (if you ever see an example that links to <code>user32</code> that will 
also work).</p>
<pre><code class="language-rust no_run noplaypen">extern &quot;stdcall&quot; {
    /// https://docs.microsoft.com/en-us/windows/console/getstdhandle
    fn GetStdHandle(nStdHandle: i32) -&gt; i32;
    /// https://docs.microsoft.com/en-us/windows/console/writeconsole
    fn WriteConsoleW(
        hConsoleOutput: i32,
        lpBuffer: *const u16,
        numberOfCharsToWrite: u32,
        lpNumberOfCharsWritten: *mut u32,
        lpReserved: *const std::ffi::c_void,
    ) -&gt; i32;
}
</code></pre>
<p>First of all, <code>extern &quot;stdcall&quot;</code>, tells the compiler that we won't use the <code>C</code> 
calling convention but use Windows calling convention called <code>stdcall</code>.</p>
<p>The next part is the functions we want to link to. On Windows, we need to link 
to two functions to get this to work: <code>GetStdHandle</code> and <code>WriteConsoleA</code>. 
<code>GetStdHandle</code> retrieves a reference to a standard device like <code>stdout</code>.</p>
<p>WriteConsole comes in two flavors, <code>WriteConsoleW</code> that takes in Unicode text 
and <code>WriteConsoleA</code> that takes ANSI encoded text. </p>
<p>Now, ANSI encoded text works fine if you only write English text, but as soon as 
you write text in other languages you might need to use special characters that 
are not possible to represent in <code>ANSI</code> but is possible in <code>utf-8</code> and our code 
will break.</p>
<p>That's why we'll convert our <code>utf-8</code> encoded text to <code>utf-16</code> encoded Unicode 
text that can represent these characters and use the <code>WriteConsoleW</code> function.</p>
<pre><code class="language-rust no_run noplaypen">#[cfg(target_os = &quot;windows&quot;)]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {

    // let's convert our utf-8 to a format windows understands
    let msg: Vec&lt;u16&gt; = message.encode_utf16().collect();
    let msg_ptr = msg.as_ptr();
    let len = msg.len() as u32;
    
    let mut output: u32 = 0;
        let handle = unsafe { GetStdHandle(-11) };
        if handle  == -1 {
            return Err(io::Error::last_os_error())
        }

        let res = unsafe { 
            WriteConsoleW(handle, msg_ptr, len, &amp;mut output, std::ptr::null()) 
            };

        if res  == 0 {
            return Err(io::Error::last_os_error());
        }

    assert_eq!(output, len);
    Ok(())
}
</code></pre>
<p>The first thing we do is to convert the text to utf-16 encoded text which 
Windows uses. Fortunately Rust has a built in function to encode our <code>utf-8</code>
encoded text to <code>utf-16</code> code points. <code>encode_utf16</code> returns an iterator over 
<code>u16</code> code points that we can collect to a <code>Vec</code>.</p>
<pre><code class="language-rust no_run noplaypen">let msg: Vec&lt;u16&gt; = message.encode_utf16().collect();
let msg_ptr = msg.as_ptr();
let len = msg.len() as u32;
</code></pre>
<p>We then get the pointer to the underlaying buffer of our <code>Vec</code> and calculate the 
length.</p>
<pre><code class="language-rust no_run noplaypen">let handle = unsafe { GetStdHandle(-11) };
   if handle  == -1 {
       return Err(io::Error::last_os_error())
   }
</code></pre>
<p>The next is a call to <code>GetStdHandle</code>. We pass in the value <code>-11</code>. The values we 
need to pass in for the different standard devices is actually documented 
together with the <code>GetStdHandle</code> documentation:</p>
<ul>
<li>Stdin: -10</li>
<li>Stdout: -11</li>
<li>StdErr: -12</li>
</ul>
<p>Now, we're lucky here, it's not that common that we find this information 
together with the documentation function we call but it's very convenient when 
we do.</p>
<p>What return codes to expect is also documented thoroughly so we handle potential 
errors here in the same way as we did for the Linux/Macos syscalls.</p>
<pre><code class="language-rust no_run noplaypen">let res = unsafe { 
    WriteConsoleW(handle, msg_ptr, len, &amp;mut output, std::ptr::null()) 
    };

if res  == 0 {
    return Err(io::Error::last_os_error());
}
</code></pre>
<p>Next up is the call to the <code>WriteConsoleW</code> function. Now that we have explained 
everything else there is nothing too fancy about this.</p>
<h2><a class="header" href="#the-highest-level-of-abstraction" id="the-highest-level-of-abstraction">The highest level of abstraction</a></h2>
<p>This is simple, most standard libraies provide this abstraction for you. In rust that would simple be:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
println!(&quot;Hello world from Stdlib&quot;);
#}</code></pre></pre>
<h2><a class="header" href="#a-note-about-complexity" id="a-note-about-complexity">A note about complexity</a></h2>
<p>There is a lot of &quot;hidden&quot; complexity when writing cross platform code at this 
level. One hurdle is to get something working, which can prove to be quite a 
challenge. Getting it to work <strong>correctly</strong> and <strong>safely</strong> while covering edge 
cases is an additional challenge. </p>
<p>Are we 100% sure that all valid <code>utf-8</code> code points which we use in Rust is valid 
<code>utf-16</code> encoded Unicode that Windows will display correctly?</p>
<p>I think so, but being 100 % sure about <a href="https://en.wikipedia.org/wiki/Comparison_of_Unicode_encodings">this might not be as easy as one might think</a>.</p>
<h1><a class="header" href="#our-finished-cross-platform-syscall" id="our-finished-cross-platform-syscall">Our finished cross platform syscall</a></h1>
<pre><pre class="playpen"><code class="language-rust">use std::io;

fn main() {
    let sys_message = String::from(&quot;Hello world from syscall!\n&quot;);
    syscall(sys_message).unwrap();
}

// and: http://man7.org/linux/man-pages/man2/write.2.html
#[cfg(not(target_os = &quot;windows&quot;))]
#[link(name = &quot;c&quot;)]
extern &quot;C&quot; {
    fn write(fd: u32, buf: *const u8, count: usize) -&gt; i32;
}

#[cfg(not(target_os = &quot;windows&quot;))]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {
    let msg_ptr = message.as_ptr();
    let len = message.len();
    let res = unsafe { write(1, msg_ptr, len) };

    if res == -1 {
        return Err(io::Error::last_os_error());
    }
    Ok(())
}

#[cfg(target_os = &quot;windows&quot;)]
#[link(name = &quot;kernel32&quot;)]
extern &quot;stdcall&quot; {
    /// https://docs.microsoft.com/en-us/windows/console/getstdhandle
    fn GetStdHandle(nStdHandle: i32) -&gt; i32;
    /// https://docs.microsoft.com/en-us/windows/console/writeconsole
    fn WriteConsoleW(
        hConsoleOutput: i32,
        lpBuffer: *const u16,
        numberOfCharsToWrite: u32,
        lpNumberOfCharsWritten: *mut u32,
        lpReserved: *const std::ffi::c_void,
    ) -&gt; i32;
}

#[cfg(target_os = &quot;windows&quot;)]
fn syscall(message: String) -&gt; io::Result&lt;()&gt; {

    // let's convert our utf-8 to a format windows understands
    let msg: Vec&lt;u16&gt; = message.encode_utf16().collect();
    let msg_ptr = msg.as_ptr();
    let len = msg.len() as u32;

    let mut output: u32 = 0;
        let handle = unsafe { GetStdHandle(-11) };
        if handle  == -1 {
            return Err(io::Error::last_os_error())
        }

        let res = unsafe { 
            WriteConsoleW(handle, msg_ptr, len, &amp;mut output, std::ptr::null()) 
            };

        if res  == 0 {
            return Err(io::Error::last_os_error());
        }

    assert_eq!(output, len);
    Ok(())
}
</code></pre></pre>
<h1><a class="header" href="#about-writing-cross-platform-abstractions" id="about-writing-cross-platform-abstractions">About writing cross platform abstractions</a></h1>
<p>If you isolate the code needed only for Linux and Macos you'll see that it's not many lines of code to write. But once you want to make a cross platform variant, the amount of code explodes. This is a problem when writing about this stuff in general, but we need some basic understanding on how the different operating systems work under the covers. </p>
<p>My experience in general is that Linux and Macos have simpler API requiring fewer lines of code, and often (but not always) the exact same call works for both systems.</p>
<p>Windows on the other hand is mor complex, requires more &quot;magic&quot; constant numbers, requires you to set up more structures to pass in, and way more lines of code. What Windows does have though are very good documentation so even though it's more work you'll also find the official documentation very good.</p>
<p>This complexity why the Rust community (other languages often has something similar) gathers around crates like <a href="https://github.com/rust-lang/libc">libc</a> which already have defined most methods and constants you need.</p>
<h2><a class="header" href="#" id=""></a></h2>
<h1><a class="header" href="#does-the-cpu-and-the-os-cooperate" id="does-the-cpu-and-the-os-cooperate">Does the CPU and the OS cooperate?</a></h1>
<p>If you would have asked me this question a year ago I would most likely answer
no. The OS operates on top of the CPU and is the &quot;gateway&quot; all other application
uses. Now, first of all, I wouldn't have thought this through, but unless you
learn how this work from the bottom up, it's not easy to know for sure.</p>
<p>What started to make me think I was very wrong was some code looking like this:</p>
<blockquote>
<p>We'll write instructions directly to the CPU so there know there are no 
abstractions, syscalls or runtime of some sort that might do some magic for us</p>
</blockquote>
<pre><pre class="playpen"><code class="language-rust">#![feature(asm)]
fn main() {
    let t = 100;
    let t_ptr: *const usize = &amp;t;
    let x = dereference(t_ptr);
    
    println!(&quot;{}&quot;, x);
}

fn dereference(ptr: *const usize) -&gt; usize {
    let res: usize;
    unsafe { 
        asm!(&quot;mov ($1), $0&quot;:&quot;=r&quot;(res): &quot;r&quot;(ptr)) 
        };
    res
}
</code></pre></pre>
<p>Here we write a <code>dereference</code> function using assembly instructions. We know there
is no way the OS is involved here.</p>
<p>As you see, this code will output <code>100</code> as expected. But let's now instead create a 
pointer with the address <code>99999999999999</code> which we know is invalid and see what 
happens when we pass that into the same function:</p>
<pre><pre class="playpen"><code class="language-rust">#![feature(asm)]
fn main() {
    let t = 99999999999999 as *const usize;
    let x = dereference(t);
    
    println!(&quot;{}&quot;, x);
}
# fn dereference(ptr: *const usize) -&gt; usize {
#     let res: usize;
#     unsafe {
#     asm!(&quot;mov ($1), $0&quot;:&quot;=r&quot;(res): &quot;r&quot;(ptr));
#     }
# 
#     res
# }
</code></pre></pre>
<p>Now we get a segmentation fault. Not surprising really, but how does the CPU
know that we're not allowed to dereference this memory?</p>
<ul>
<li>Does the CPU ask the OS if this process is allowed to access this memory location?</li>
<li>Won't that be very slow? </li>
<li>How does the CPU know that it has an OS running on top of it at all? D</li>
<li>Does all CPUs know what a segmentation fault is? </li>
<li>Does the CPU have any concept of memory segmentation at all? </li>
<li>Why do we get an error message at all and not just a crash?</li>
</ul>
<h2><a class="header" href="#down-the-rabbit-hole" id="down-the-rabbit-hole">Down the rabbit hole</a></h2>
<p>Yes, this is a small rabbit hole. It turns out that there
os some kind of corporation, but maybe not the way you naively would think.</p>
<p>Many modern CPUs provide some infrastructure that Operating Systems need to 
implement and give us the security and stability we expect. Actually, most 
advanced CPUs provide a lot more options than operating systems like Linux, BSD and
Windows actually uses.</p>
<p>There is especially two that I want to address here:</p>
<ol>
<li>How the CPU prevents us from accessing memory we're not supposed to access</li>
<li>How the CPU handles asynchronous events like I/O</li>
</ol>
<blockquote>
<p>If you want to know more about how this works in detail I will absolutely
recommend that you give <a href="https://os.phil-opp.com/">Philipp Oppermann excelent series</a>
a read. It's extremely well written and will answer all these questions and many more.</p>
</blockquote>
<h2><a class="header" href="#how-does-the-cpu-prevent-us-from-accessing-memory-were-not-supposed-to" id="how-does-the-cpu-prevent-us-from-accessing-memory-were-not-supposed-to">How does the CPU prevent us from accessing memory we're not supposed to?</a></h2>
<p>As I mentioned, modern CPUs have already some definition of basic concepts. Some
concepts the CPU is aware of are (there are more but we can't cover everything here):</p>
<ul>
<li>Virtual memory </li>
<li>Page table</li>
<li>Page fault</li>
<li>Exceptions</li>
<li><a href="https://en.wikipedia.org/wiki/Protection_ring">Privelege level</a></li>
</ul>
<p>Exactly how this works will differ depending on the exact CPU so we'll treat them 
in general terms here.</p>
<p>Most modern CPUs however has a MMU (Memory Management Unit). This is a part of the
CPU (often etched on the same dye even). The MMUs job is to translate between
the virtual address we use in our programs, to a physical address.</p>
<p>When the OS starts a process (like our program) it sets up a page table for our
process, and makes sure a special register on the CPU points to this page table.</p>
<p>Now, when we try to dereference <code>t_ptr</code> in the code above, the address is at some point
sent to the MMU for translation, which looks it up in the page table which translates
it to the physical address.</p>
<p>In this case it will point to a memory address on our stack that holds the value <code>100</code>.</p>
<p>When we pass in <code>99999999999999</code> and ask it to fetch what's stored at that address 
(which is what dereferencing does) it looks for the translation in the page table but
can't find it.</p>
<p>The CPU then treats this as a <code>page fault</code>. </p>
<p>At boot, the OS at provided the CPU with a Interrupt Descriptor Table. This table
has a predefined format where the OS has to provide handlers for the predefined 
exceptions the CPU can encounter.</p>
<p>Since the OS provided ha pointer to a function that handles <code>Page Fault</code> the CPU 
jumps to that function and thereby hands over control to the Operating System. </p>
<p>The OS then prints a nice message for us letting us know that we encountered 
what it calls a <code>segmentation fault</code>. This message will therefore wary depending on the OS you 
run the code on.</p>
<h2><a class="header" href="#but-cant-we-just-change-the-page-table-in-the-cpu" id="but-cant-we-just-change-the-page-table-in-the-cpu">But can't we just change the page table in the CPU?</a></h2>
<p>Now this is where <code>Privelege Level</code> comes in. Most modern operating systems operate with two <code>Ring Levels</code>. Ring 0, the kernel space, and Ring 3, user space.</p>
<p><img src="./images/priv_rings.png" alt="Privelege rings" /></p>
<p>Most CPUs has a concept of more rings that what most modern operating systems use. This has historical reasons, which is also why <code>Ring 0</code> and <code>Ring 3</code> are used (and not 1, 2).</p>
<p>Now every entry in the page table has additional information about it, amongst that information is the information about what ring it belongs to. This is set up when your OS boots up,</p>
<p>Code executed in <code>Ring 0</code> has almost unrestricted access to external devices, memory and is free to change registers that provide security at the hardware level.</p>
<p>Now, the code you write in <code>Ring 3</code> will typically have extremely restricted access to I/O and certain CPU registers (and instructions). Trying to issue an instruction or setting a register from <code>Ring 3</code> to change the <code>page table</code> will be prevented already at the CPU level, which then treats this as an exception and jumps to the handler provided by the OS.</p>
<p>This is also the reason why you have no other choice than to cooperate with the OS and handle I/O tasks through syscalls. The system wouldn't be very secure if this wasn't the case.</p>
<h1><a class="header" href="#interrupts-firmware-and-io" id="interrupts-firmware-and-io">Interrupts, Firmware and I/O</a></h1>
<p>We're nearing an end of the general CS subjects in the book, and we'll start
to dig our way out of the rabbit hole soon.</p>
<p>This part tries to tie things together and look at how the whole computer works
as a system to handle I/O and concurrency. </p>
<p>Let's get to it!</p>
<h2><a class="header" href="#a-simplified-overview" id="a-simplified-overview">A simplified overview</a></h2>
<p>Let's go through some of the steps where we imagine that we read from a
network card:</p>
<p><img src="./images/AsyncBasicsSimplified.png" alt="Simplified Overview" /></p>
<blockquote>
<p><strong>Disclaimer</strong>
We're making things simple here. This is a rather complex operation but we'll
choose the operations that interest us and skip some along the way.</p>
</blockquote>
<h2><a class="header" href="#1-our-code" id="1-our-code">1. Our code</a></h2>
<p>We register a socket. This happens by issuing a <code>syscall</code> to the OS. Depending
on the OS we either get a  <code>file descriptor</code> (Macos/Linux) or a <code>socket</code> (Windows).</p>
<p>The next step is that we register our interest in <code>read</code> events on that socket.</p>
<h2><a class="header" href="#2-we-register-events-with-the-os" id="2-we-register-events-with-the-os">2. We register events with the OS</a></h2>
<p>The operating systems we focus on handles this in one of three ways:</p>
<ol>
<li>We tell the operating system that we're interested in <code>Read</code> events but we want
to wait for it to happen by <code>yielding</code> control over our thread to the OS. The OS
then suspends our thread by storing the register state and switch to some other 
thread. </li>
</ol>
<p>From our perspective this will be blocking our thread until we have data to read.</p>
<ol start="2">
<li>
<p>We tell the operating system that we're interested in <code>Read</code> events but we
just want a handle to a the task which we can <code>poll</code> to check if the event is
ready or not. The OS will not suspend our thread.</p>
</li>
<li>
<p>We tell the operating system that we are probably going to be interested in 
many events, but we want to subscribe to one event queue. When we <code>poll</code> this
queue it will block until one or more event occurs.</p>
</li>
</ol>
<blockquote>
<p>My next book will be about about alternative C since that is a very interesting
model of handling I/O events that's going to be important later on to understand 
why Rusts concurrency abstractions are modeled the way they are. Of that reason 
we won't cover this in detail here.</p>
</blockquote>
<h2><a class="header" href="#3-the-network-card" id="3-the-network-card">3. The Network Card</a></h2>
<blockquote>
<p>We're skipping some steps here but it's not very vital to our understanding. </p>
</blockquote>
<p>Meanwhile on the network card there is a small microcontroller running 
specialized firmware. We can imagine that this microcontroller is polling in a
busy loop checking if any data is incoming. </p>
<blockquote>
<p>The exact way the Network Card handles it internals can be different from this
(and most likely are). The important part is that there is a simple CPU running
on the network card doing work to check if there is incoming events.</p>
</blockquote>
<p>Once the firmware registers incoming data it issues a Hardware Interrupt.</p>
<h2><a class="header" href="#4-hardware-interrupt" id="4-hardware-interrupt">4. Hardware Interrupt</a></h2>
<blockquote>
<p>This is a very simplified explanation. If you're interested in knowing more
about how this works I can't recommend Robert Mustacchi's excellent article
<a href="https://www.joyent.com/blog/virtualizing-nics">Turtles on the wire: understanding how the OS uses the modern NIC</a> enough.</p>
</blockquote>
<p>Modern CPUs have a set of <code>Interrupt Request Lines</code> for it to handle events that occur from
external devices. A CPU has a fixed set of interrupt lines.</p>
<p>A hardware interrupt is an electrical signal that can occur at <em>any time</em>. The 
CPU immediately <strong>interrupts</strong> its normal workflow to handle the interrupt by
saving the state of it's registers and look up the interrupt handler to run in
the Interrupt Descriptor Table</p>
<h2><a class="header" href="#5-interrupt-handler" id="5-interrupt-handler">5. Interrupt Handler</a></h2>
<p>The <a href="https://en.wikipedia.org/wiki/Interrupt_descriptor_table">Interrupt Descriptor Table</a> is a table that in essence points to a handler function for a specific interrupt. The handler function for a Network Card would typically be registered and handled by a <code>driver</code> for that card.</p>
<blockquote>
<p>The IDT is not stored on the CPU as it might seem in the diagram. It's located
in a fixed and know location in main memory. The CPU only holds a pointer to the
table in one of it's registers.</p>
</blockquote>
<h2><a class="header" href="#6-writing-the-data" id="6-writing-the-data">6. Writing the data</a></h2>
<p>This is a step that might vary a lot depending on the CPU and the firmware on the
network card. If the Network Card and the CPU supports <a href="https://en.wikipedia.org/wiki/Direct_memory_access">Direct Memory Access</a> (which should be the standard on all modern systems today) the Network Card will write data directly to a set of buffers the OS already has set up in main memory. In such a system the <code>firmware</code> on the Network Card might issue an <code>Interrupt</code> when the data is written to memory. <code>DMA</code> is very efficient
since the CPU is only notified when the data is already in memory. On older systems the
CPU needed to devote often substantial time to handle the data transfer from the
network card.</p>
<p>The DMAC (Direct Memory Access Controller) is just added since in such a system,
it would control the access to memory. It's not part of the CPU per se as in the
diagram above. It's we're deep enough in the rabbit hole now and this is not really important for us right now so let's move on.</p>
<h2><a class="header" href="#7-the-driver" id="7-the-driver">7. The driver</a></h2>
<p>The <code>driver</code> would normally handle the communication between the OS and the Network Card.
At <em>some point</em> the buffers are filled, and the network card issues an <code>Interrupt</code>. The CPU then jumps to the handler of that interrupt. The interrupt handler for this exact type
of interrupt is registered by the driver, so it's actually the driver that handles this event and in turn informs the kernel that the data is ready to be read. </p>
<h2><a class="header" href="#8-reading-the-data" id="8-reading-the-data">8. Reading the data</a></h2>
<p>Depending on whether we chose alternative A, B or C the OS will:</p>
<ol>
<li>Wake our thread</li>
<li>Return <code>Ready</code> on the next <code>poll</code></li>
<li>Wake the thread and return a <code>Read</code> event for the handler we registered.</li>
</ol>
<h2><a class="header" href="#interrupts" id="interrupts">Interrupts</a></h2>
<p>As I hinted at above there are two kinds of interrupts:</p>
<ol>
<li>Hardware Interrupts</li>
<li>Software Interrupts</li>
</ol>
<p>They are very different in nature.</p>
<h3><a class="header" href="#hardware-interrupts" id="hardware-interrupts">Hardware Interrupts</a></h3>
<p>Hardware interrupts are created by sending an electrical signal through an <a href="https://en.wikipedia.org/wiki/Interrupt_request_(PC_architecture)#x86_IRQs">Interrupt Request Line (IRQ)</a>. These are hardware lines signaling the CPU directly. </p>
<h3><a class="header" href="#software-interrupts" id="software-interrupts">Software Interrupts</a></h3>
<p>This is interrupts issued from software instead of hardware. As in the case of a hardware interrupt the CPU jumps to the Interrupt Descriptor Table and runs the handler for the specified interrupt.</p>
<h3><a class="header" href="#firmware" id="firmware">Firmware</a></h3>
<p>Firmware doesn't get much attention from most of us, however, they're a crucial part of the world we live in. They run on all kinds of hardware, and has all kinds of strange and peculiar ways to make the computer we program on work.</p>
<p>When I think about firmware, I think about the scenes from Star Wars where they walk into a bar with all kinds of strange and obscure creatures. I imagine the world of firmware is much like this, few us of know what they do or how they work on a particular system.</p>
<p>Now, firmware needs a microcontroller or similar to be able to work. Even the CPU has firmware which makes it work. That means there are many more small &quot;CPUs&quot; on our system than the cores we program against.</p>
<p>Why is this important? Well, you remember that concurrency is all about efficiency right? We'll since we have many CPU's already doing work for us on our system, one of our concerns is to not replicate or duplicate that work when we write code.</p>
<p>If a network card has firmware that continually checks if new data has arrived, it's pretty wasteful if we duplicate that by letting our CPU continually check if new data arrives as well. It's much better if we either check once in a while or even better, gets notified when data has arrived for us.</p>
<h1><a class="header" href="#strategies-for-handling-io" id="strategies-for-handling-io">Strategies for handling I/O</a></h1>
<p>Before we dive into Writing some code we'll finish off this part of the book talking a bit about different strategies of handling I/O and concurrency. Now, just note that I'm covering I/O in general here, but I use network communication as the main example. Different strategies can have different strengths depending on what type of I/O we're talking about.</p>
<h2><a class="header" href="#the-perfect-solution" id="the-perfect-solution">The perfect solution</a></h2>
<p>Let's start off by picturing a perfect world, and how the most efficient way of handling I/O in a concurrent manner could look like.</p>
<p>If we go back to this model and think it through based on the knowledge we now have. If we want to use every CPU cycle the best way possible how would we design this?</p>
<p><img src="./images/AsyncBasicsSimplified.png" alt="overview" /></p>
<p>The best way would be the following:</p>
<ol>
<li>We give the Network Card a message that we want to be notified <strong>immediately</strong> when new data has arrived for us.</li>
<li>The network card is hyper optimized in the way it checks for data, it's firmware makes sure of that.</li>
<li>As soon as some data has arrived for us the Network Card let's us know</li>
<li>We either finish what we're doing or handle that data immediately</li>
</ol>
<p>This is super simplified, and probably not realistic. The idea here is that the logic that is closest to the &quot;problem&quot; and has the best hardware support to handle it does the work. Also, we only get notified when we are expecting something, not on every event that might occur. These assumptions are not very realistic.</p>
<h2><a class="header" href="#1-using-os-threads" id="1-using-os-threads">1. Using OS threads</a></h2>
<p>Now one way of accomplishing this is letting the OS take care of everything for us. We do this by simply spawning a new OS thread for each task we want to accomplish and write code like we normally would.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Simple</li>
<li>Easy to code</li>
<li>Reasonably performant</li>
<li>You get parallelism for free</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>OS level threads come with a rather large stack. If you have many tasks happening simultaneously (like in a webserver under heavy load) you'll run out of memory pretty soon.</li>
<li>There are a lot of syscalls involved this can be pretty costly</li>
<li>The OS has many things it needs to handle. It might not switch back to your thread as fast as you'd wish</li>
<li>The OS doesn't know which tasks to prioritize, and you might want to give som tasks a higher priority than others.</li>
</ul>
<h2><a class="header" href="#2-green-threads" id="2-green-threads">2. Green threads</a></h2>
<p>Another common way of handling this is green threads. Languages like GO uses this to great success. In many ways this is similar to what the OS does but the runtime can be better adjusted and suited to your specific needs.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Simple to use for the user. The code will look like it does when using OS threads</li>
<li>Reasonably performant</li>
<li>Abundant memory usage is less of a problem</li>
<li>You are in full control over how threads are scheduled and if you want you can prioritize them differently.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>You need a runtime, and by having that you are duplicating part of the work the OS already does. The runtime will have a cost which in some cases can be substantial.</li>
<li>Can be difficult to implement in a flexible way to handle a wide set of tasks</li>
</ul>
<h2><a class="header" href="#3-poll-based-event-loops-supported-by-the-os" id="3-poll-based-event-loops-supported-by-the-os">3. Poll based event loops supported by the OS</a></h2>
<p>The third way we're covering today is the one that most closely matches our <em>ideal</em> solution. In this solution the we register an interest in an event, and then let the OS tell us when it's ready. </p>
<p>The way this works is that we tell the OS that we're interested in knowing when data is arriving for us on the network card. The network card issues an interrupt when something has happened in which the driver let's the OS know that the data is ready. The OS let's us know that data is ready for us to read.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Very little work is duplicated which makes it very performant</li>
<li>It's very efficient</li>
<li>Gives us the maximum amount of flexibility to decide how to handle the events that occurs</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Different operating systems have different ways of handle these kind of queues. Some of them are difficult to reconcile with each other. Some operating systems has limitations on what I/O operations support this method.</li>
<li>Great flexibility comes with a good deal of complexity</li>
<li>Difficult to write an abstraction layer that accounts for the differences between the operating systems without introducing unwanted costs, and at the same time provide a ergonomic API.</li>
</ul>
<h2><a class="header" href="#final-note" id="final-note">Final note</a></h2>
<p>The Node runtime uses a combination of both 1 and 3, but tries to force all I/O to use alternative 3. This is also part of the reason why Node is so good at handle many connections concurrently.</p>
<p>Rusts async story is modeled around option 3, and some of the reason it has taken a long time is related to the <em>cons</em> of this method. Most notably the last point.</p>
<h1><a class="header" href="#implementing-the-node-eventloop" id="implementing-the-node-eventloop">Implementing the Node Eventloop</a></h1>
<p>Now we've finally come to the part of this book where we will write som more code.</p>
<p>The Node event loop is a complex piece of software developed over many years. We
will have to simplify things a lot. There are many edge cases that we won't cover,
and there are many intricacies that we'll not mention or implement. </p>
<p>However, I will try to implement the parts that are important for us to understand Node better and most importantly use it as an example where we can use our knowledge from the previous chapters to make something that actually works.</p>
<p>What we will do is to look at how it conceptually works. Our main goal here is
to explore async concepts, using Node as an example is part curiosity and part
for fun.</p>
<p>We want to write something like this:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
print(&quot;First call to read test.txt&quot;);
Fs::read(&quot;test.txt&quot;, |result| {
    let text = result.into_string().unwrap();
    let len = text.len();
    print(format!(&quot;First count: {} characters.&quot;, len));

print(r#&quot;I want to create a &quot;magic&quot; number based on the text.&quot;#);
Crypto::encrypt(text.len(), |result| {
    let n = result.into_int().unwrap();
    print(format!(r#&quot;&quot;Encrypted&quot; number is: {}&quot;#, n));
    })
});


print(&quot;Registering immediate timeout 1&quot;);
set_timeout(0, |_res| {
    print(&quot;Immediate1 timed out&quot;);
});
print(&quot;Registering immediate timeout 2&quot;);
set_timeout(0, |_res| {
    print(&quot;Immediate2 timed out&quot;);
});
print(&quot;Registering immediate timeout 3&quot;);
set_timeout(0, |_res| {
    print(&quot;Immediate3 timed out&quot;);
});

// let's read the file again and display the text
print(&quot;Second call to read test.txt&quot;);
Fs::read(&quot;test.txt&quot;, |result| {
    let text = result.into_string().unwrap();
    let len = text.len();
    print(format!(&quot;Second count: {} characters.&quot;, len));

    // aaand one more time but not concurrently
    print(&quot;Third call to read test.txt&quot;);
    Fs::read(&quot;test.txt&quot;, |result| {
        let text = result.into_string().unwrap();
        print_content(&amp;text, &quot;file read&quot;);
    });
});

print(&quot;Registering a 3000 and a 500 ms timeout&quot;);
set_timeout(3000, |_res| {
    print(&quot;3000ms timer timed out&quot;);
    set_timeout(500, |_res| {
        print(&quot;500ms timer(nested) timed out&quot;);
    });
});

print(&quot;Registering a 1000 ms timeout&quot;);
set_timeout(1000, |_res| {
    print(&quot;SETTIMEOUT&quot;);
});

// `http_get_slow` let's us define a latency we want to simulate
print(&quot;Registering http get request to google.com&quot;);
Io::http_get_slow(&quot;http//www.google.com&quot;, 2000, |result| {
    let result = result.into_string().unwrap();
    print_content(result.trim(), &quot;web call&quot;);
});
#}</code></pre></pre>
<p>And since this is just a runtime we write to learn we get this output showing us what happens when we run that code:</p>
<pre><code>Thread: main     First call to read test.txt
Thread: main     Registering immediate timeout 1
Thread: pool3    recived a task of type: File read
Thread: main     Registered timer event id: 2
Thread: main     Registering immediate timeout 2
Thread: main     Registered timer event id: 3
Thread: main     Registering immediate timeout 3
Thread: main     Registered timer event id: 4
Thread: main     Second call to read test.txt
Thread: main     Registering a 3000 ms timeout
Thread: pool2    recived a task of type: File read
Thread: main     Registered timer event id: 6
Thread: main     Registering a 1000 ms timeout
Thread: main     Registered timer event id: 7
Thread: main     Registering http get request to google.com
Thread: main     Event with id: 8 registered.
Thread: main     ===== TICK 1 =====
Thread: main     Immediate1 timed out
Thread: main     Immediate2 timed out
Thread: main     Immediate3 timed out
Thread: main     ===== TICK 489 =====
Thread: main     SETTIMEOUT
Thread: pool2    finished running a task of type: File read.
Thread: pool3    finished running a task of type: File read.
Thread: main     ===== TICK 1057 =====
Thread: main     First count: 39 characters.
Thread: main     I want to create a &quot;magic&quot; number based on the text.
Thread: main     Second count: 39 characters.
Thread: main     Third call to read test.txt
Thread: pool3    recived a task of type: Encrypt
Thread: pool2    recived a task of type: File read
Thread: epoll    epoll event 8 is ready
Thread: main     ===== TICK 1124 =====

===== THREAD main START CONTENT - WEB CALL =====
HTTP/1.1 302 Found
Server: CowbHTTP/1.1 302 Found
Server: CowbHTTP/1.1 302 Found
Server: Cowboy
Date: Sun, 22 Sep 2019 15:17HTTP/1.1 302 Found
Server: Cowboy
Date: Sun, 22 Sep 2019 15:17:52 GMT
Connection: close
Content-Type: text/html;charset=utf-HTTP/1.1 302 Found
Server: Cowboy
Date: Sun, 22 Sep 2019 15:17:52 GMT
Connection: close
Content-Type: text/html;charset=utf-8
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: OPTIONS
Access-Control-Allow-Methods: GET
Access-Control-AlloHTTP/1.1 302 Found
Server: Cowboy
Date: Sun, 22 Sep 2019 15:17:52 GMT
Connection: close
Content-Type: text/html;charset=utf-8
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: OPTIONS
Access-Control-Allow-Methods: GET
Access-Control-Allow-Methods: POST
Location: http://http/www.google.com
X-Xss-Protection: 1; mode=block
X-Content-Type-Options: nosniff
X-Frame-Options: SAMEORIGIN
Content-Length: 0
Via: 1.1 vegur


===== END CONTENT =====

Thread: pool3    finished running a task of type: Encrypt.
Thread: main     ===== TICK 1330 =====
Thread: main     &quot;Encrypted&quot; number is: 63245986
Thread: main     ===== TICK 1597 =====
Thread: main     3000ms timer timed out
Thread: main     Registered timer event id: 11
Thread: main     ===== TICK 1882 =====
Thread: main     500ms timer(nested) timed out
Thread: pool2    finished running a task of type: File read.
Thread: main     ===== TICK 2171 =====

===== THREAD main START CONTENT - FILE READ =====
Hello world! This is a text to encrypt!
===== END CONTENT =====

Thread: main     FINISHED
</code></pre>
<p>In the following chapters I'll walk you through what's going on here but right from the start we can notice some important things going on here:</p>
<p>The code we write has a lot of simiarities with Javascript and looks very little like idiomatic Rust code.</p>
<p>When you look through the events you'll see they happen in a different order that we wrote them. That tells us that there is some concurrency going on here.</p>
<p>The next chapters will explain everything going on here, so relax, get a cup of tea and stay focused as we explain everything.</p>
<h1><a class="header" href="#what-is-node" id="what-is-node">What is Node?</a></h1>
<p>We have to start with a short explanation of what Node is, just so we're on the same page.</p>
<p>Node is a Javascript runtime allowing Javascript to run on you'r desktop (or server). Javascript was originally designed as a scripting language for the browser which also means that Javascript in itself needs some runtime to interpret it.</p>
<p>Javascript has one advantage from a language design perspective: Everything is designed to be handled asynchronously. An as you know by now, this pretty crucial if we want to make the most out of our hardware especially if you have a lot of I/O operations to take care of.</p>
<p>One such scenario is a Webserver. Webservers handle a lot of I/O tasks whether it's reading from the file system or communicating via the network card.</p>
<h2><a class="header" href="#why-node" id="why-node">Why Node</a></h2>
<ul>
<li>Javascript is unavoidable when doing web development. Using Javascript on the server allowed programmers to use the same language both places.</li>
<li>There is a potential for code reuse between the server and the front end</li>
<li>The design of Node allows it to make very perfromant web servers</li>
<li>Working with Json and APIs is very easy when you only deal with Javascript</li>
</ul>
<h2><a class="header" href="#myths-and-helpfull-facts" id="myths-and-helpfull-facts">Myths and helpfull facts</a></h2>
<p>Let's start off by debunking some myths that might make it easier to follow along when we start to code.</p>
<h3><a class="header" href="#the-javascript-eventloop" id="the-javascript-eventloop">The Javascript Eventloop</a></h3>
<p>Javascript is a scripting languange and can't do much on it's own. It doesn't hava an event loop. Now in a web browser, the browser provides a runtime, which includes an event loop. And on the server, Node provides this functinality. You might say that Javascript as a language would be difficult to run (due to it's callback based model) without some sort of event loop, that's beside the point.</p>
<h3><a class="header" href="#node-isnt-multithreaded" id="node-isnt-multithreaded">Node isn't multithreaded</a></h3>
<p>This isn't true. But the part of Node that &quot;progresses&quot; your code, does indeed run on a single thread. When we say &quot;don't block the event loop&quot; we're referring to this thread since that will prevent Node to progress other tasks.</p>
<p>We'll see exactly why blocking this thread is a problem and how that's
handled.</p>
<h3><a class="header" href="#the-v8-javascript-engine" id="the-v8-javascript-engine">The V8 javascript engine</a></h3>
<p>Now, this is where we need to focus a bit. The V8 engine is a javascript JIT compiler. That means that when you write a <code>for</code> loop, the V8 engine translates this to instructions that run on you'r CPU. There are many javascript engines, but Node was originally implemented on top of the V8 engine.</p>
<p>The V8 engine itself can't do much useful for us, it just interprets our Javascript. It can't do I/O, set up a runtime or anything like that. Writing Javascript only with V8 will be a very limited experience.</p>
<blockquote>
<p>Since we write Rust as you saw on the top, we'll not cover the part of translating javascript. We'll just focus on how Node works and handles concurrency since that's our main focus right now.</p>
</blockquote>
<h1><a class="header" href="#whats-our-plan" id="whats-our-plan">What's our plan</a></h1>
<p>For our plan to work we need a runtime to run our &quot;javascript&quot;.</p>
<p>Let's first start with our &quot;javascript&quot;:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
/// Think of this function as the javascript program you have written
fn javascript() {
    print(&quot;First call to read test.txt&quot;);
    Fs::read(&quot;test.txt&quot;, |result| {
        let text = result.into_string().unwrap();
        let len = text.len();
        print(format!(&quot;First count: {} characters.&quot;, len));

        print(r#&quot;I want to create a &quot;magic&quot; number based on the text.&quot;#);
        Crypto::encrypt(text.len(), |result| {
            let n = result.into_int().unwrap();
            print(format!(r#&quot;&quot;Encrypted&quot; number is: {}&quot;#, n));
        })
    });


    print(&quot;Registering immediate timeout 1&quot;);
    set_timeout(0, |_res| {
        print(&quot;Immediate1 timed out&quot;);
    });
    print(&quot;Registering immediate timeout 2&quot;);
    set_timeout(0, |_res| {
        print(&quot;Immediate2 timed out&quot;);
    });
    print(&quot;Registering immediate timeout 3&quot;);
    set_timeout(0, |_res| {
        print(&quot;Immediate3 timed out&quot;);
    });
    // let's read the file again and display the text
    print(&quot;Second call to read test.txt&quot;);
    Fs::read(&quot;test.txt&quot;, |result| {
        let text = result.into_string().unwrap();
        let len = text.len();
        print(format!(&quot;Second count: {} characters.&quot;, len));

        // aaand one more time but not in parallell.
        print(&quot;Third call to read test.txt&quot;);
        Fs::read(&quot;test.txt&quot;, |result| {
            let text = result.into_string().unwrap();
            print_content(&amp;text, &quot;file read&quot;);
        });
    });

    print(&quot;Registering a 3000 and a 500 ms timeout&quot;);
    set_timeout(3000, |_res| {
        print(&quot;3000ms timer timed out&quot;);
        set_timeout(500, |_res| {
            print(&quot;500ms timer(nested) timed out&quot;);
        });
    });

    print(&quot;Registering a 1000 ms timeout&quot;);
    set_timeout(1000, |_res| {
        print(&quot;SETTIMEOUT&quot;);
    });

    // `http_get_slow` let's us define a latency we want to simulate
    print(&quot;Registering http get request to google.com&quot;);
    Io::http_get_slow(&quot;http//www.google.com&quot;, 2000, |result| {
        let result = result.into_string().unwrap();
        print_content(result.trim(), &quot;web call&quot;);
    });
}

fn print(t: impl std::fmt::Display) {
    println!(&quot;Thread: {}\t {}&quot;, current(), t);
}

fn print_content(t: impl std::fmt::Display, descr: &amp;str) {
    println!(
        &quot;\n===== THREAD {} START CONTENT - {} =====&quot;,
        current(),
        descr.to_uppercase()
    );
    println!(&quot;{}&quot;, t);
    println!(&quot;===== END CONTENT =====\n&quot;);
}
#}</code></pre></pre>
<p>Next, let's feed this code into our runtime:</p>
<pre><pre class="playpen"><code class="language-rust">fn main() {
    let mut rt = Runtime::new();
    rt.run(javascript);
}
</code></pre></pre>
<p>Now before we go on to implement our runtime we'll need some helper functions:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
fn print(t: impl std::fmt::Display) {
    println!(&quot;Thread: {}\t {}&quot;, current(), t);
}

fn print_content(t: impl std::fmt::Display, descr: &amp;str) {
    println!(
        &quot;\n===== THREAD {} START CONTENT - {} =====&quot;,
        current(),
        descr.to_uppercase()
    );
    println!(&quot;{}&quot;, t);
    println!(&quot;===== END CONTENT =====\n&quot;);
}

fn current() -&gt; String {
    thread::current().name().unwrap().to_string()
}

#}</code></pre></pre>
<p>Here we define three functions:</p>
<p><code>print</code> which prints out a message that first tells us what thread the message is beeing outputted from, and then a message we provide:</p>
<p><code>print_content</code> does the same as <code>print</code> but is a way for us to print out more than a message in a nice way.</p>
<p><code>current</code> is just a shortcut for us to get the name of the current thread. Since we want to track what's happening where we're going to need to print out what thread is issuing what output so this will avoid cluttering up our code too much along the way.</p>
<p>Next we pull in some modules from the standard library and we refer to an external library called <code>minimio</code></p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
use std::collections::{BTreeMap, HashMap};
use std::fmt;
use std::fs;
use std::io::{Read, Write};
use std::sync::mpsc::{channel, Receiver, Sender};
use std::thread::{self, JoinHandle};
use std::time::{Duration, Instant};

use minimio;
#}</code></pre></pre>
<h2><a class="header" href="#minimio" id="minimio">Minimio</a></h2>
<p>Minimio is a cross platform epoll/kqueue/IOCP based event loop that we will cover in the next book. I originally included it here but implementing that for three arcitectures is pretty interesting and needed more space than would fit in this book.</p>
<p>Most modern I/O eventloops uses a cross platform library like this. In Rust we have <a href="https://github.com/tokio-rs/mio"><code>mio</code></a>, Node uses <a href="https://github.com/libuv/libuv"><code>libuv</code></a> and there are several more. However, creating a cross platform general eventloop is pretty challenging since Windows and Unix has different ways of handeling events. We'll talk much more about this in the next book but let's just make a note of it here.</p>
<p>Let's briefly cover the how this works and why we need this in Node:</p>
<h3><a class="header" href="#epoll" id="epoll">Epoll</a></h3>
<p><code>Epoll</code> is the Linux way of implementing an event queue. In terms of functionality it has a lot of common with <code>Kqueue</code>. On a high level these abstractions provide us with this functionality:</p>
<ol>
<li>A handle to an event queue</li>
<li>A way for us to register interest for events on a file descriptor and place it in this queue</li>
<li>A way for us to wait for this event to occur by letting the OS suspend our thread and wake us up when event is ready</li>
</ol>
<h3><a class="header" href="#kqueue" id="kqueue">Kqueue</a></h3>
<p><code>Kqueue</code> is the Macos way of implementing an event queue, well, actually it's the BSD way of doint this that Macos uses. In terms of high level functionality it's similar to <code>Epoll</code>.</p>
<p>The differences are in how you interact with the queues and there are some differences in functionality but for the normal use case they are similar enough.</p>
<h3><a class="header" href="#iocp" id="iocp">IOCP</a></h3>
<p><code>IOCP</code> or Input Output Completion Ports is the way Windows handles this type of event queue. This type of queue works differently from <code>epoll</code> and <code>kqueue</code>. The biggest difference in terms of functionality is that <code>epoll</code> and <code>kqueue</code> lets you know when an event is ready (i.e. some data is ready to be read). We call this form of model a <code>readiness based</code> model.</p>
<p>Windows on the other hand uses a <code>completion based</code> model. This means it will let you know when an event has <code>Completed</code>. Now this might sound like a minor difference but it's not, especially when you want to write a library.</p>
<p>The major difficulty is that you either need to get <code>epoll</code> or <code>kqueue</code> to behave like they're <code>completion based</code> or you'll have to try to get Windows to behave in a <code>readiness based</code> manner. The latter is the way <code>wepoll</code> and <code>mio</code> does it but this change is very recent and uses a few undocumented parts of the Windows API.</p>
<h2><a class="header" href="#important-things-to-note-before-reading-further" id="important-things-to-note-before-reading-further">Important things to note before reading further</a></h2>
<p>We have to explain some things right here to prepare you for the rest.</p>
<p>We are using a callback based model, as is Node. </p>
<p>Our code here is mostly calling functions that register an event, and stores a
callback to be run when the event is ready.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
set_timeout(0, |_res| {
    print(&quot;Immediate1 timed out&quot;);
});
#}</code></pre></pre>
<p>What happens here is that we register interest in a <code>timeout</code> event. And we register
the callback <code>|_res| { print(&quot;Immediate1 timed out&quot;); }</code>. Now the parameter <code>_res</code> is
an argument that is passed in to our callback. In javascript it would be left out, but
since we use a typed language we have created a type called <code>Js</code>.</p>
<p><code>Js</code> is an enum that represents Javascript types. In the case of <code>set_timeout</code> it's
<code>Js::undefined</code>. In the case of <code>Fs::read</code> it's an <code>Js::String</code> and so on.</p>
<p>Now this callback is given an <strong>unique Id</strong> and is stored until the event occurs and
we invoke the callback and pass in any arguments we might have. In the case of <code>Fs::read</code>
that would be the text representation of the file we read.</p>
<h3><a class="header" href="#nodes-eventloops" id="nodes-eventloops">Nodes eventloop(s)</a></h3>
<p>Node internally divides it's real work into two categories:</p>
<h4><a class="header" href="#io-bound-tasks" id="io-bound-tasks">I/O bound tasks</a></h4>
<p>Are handleded by the cross platform epoll/kqueue/IOCP event queue implemented in <code>libuv</code> and in our case <code>minimio</code>.</p>
<h4><a class="header" href="#cpu-bound-tasks" id="cpu-bound-tasks">CPU bound tasks</a></h4>
<p>Are handeled by an threadpool. The default size of this threadpool is 4 threads, but that can be configured by the Node runtime.</p>
<p>I/O tasks which can't be handled by the cross platform eventqueue is also handled here which is the case with file reads which we use in our example.</p>
<p>Most C++ extensions for Node uses this threadpool to perform their work and that is one of many reasons they are used for CPU heavy tasks.</p>
<h1><a class="header" href="#the-main-loop" id="the-main-loop">The main loop</a></h1>
<p>Before we implement the eventlopp we need to set up our Runtime so we can save all our state there:</p>
<p>When we're finished, our Runtime struct will look like this: </p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
pub struct Runtime {
    
}

#}</code></pre></pre>
<p>Don't worry, we'll fill in the fields as we go along but I didn't want you to 
stop now an try to figure out what everything is.</p>
<p>Let's get back on track. And talk a bit about the eventloop, which probably is 
the most interesting part of code in this book since there has been som much
written about it:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
impl Runtime {
    pub fn run(&amp;mut self, f: impl Fn()) {
        let rt_ptr: *mut Runtime = self;
        unsafe { RUNTIME = rt_ptr as usize };

        let mut timers_to_remove = vec![]; // avoid allocating on every loop
        let mut ticks = 0; // just for us priting out

        // First we run our &quot;main&quot; function
        f();

        // ===== EVENT LOOP =====
        while self.pending_events &gt; 0 {
            ticks += 1;

            // ===== 2. TIMERS =====
            self.effectuate_timers(&amp;mut timers_to_remove);

            // NOT PART OF LOOP, JUST FOR US TO SEE WHAT TICK IS EXCECUTING
            if !self.callbacks_to_run.is_empty() {
                print(format!(&quot;===== TICK {} =====&quot;, ticks));
            }

            // ===== 2. CALLBACKS =====
            // Timer callbacks and if for some reason we have postponed callbacks
            // to run on the next tick. Not possible in our implementation though.
            self.run_callbacks();

            // ===== 3. IDLE/PREPARE =====
            // we won't use this

            // ===== 4. POLL =====
            // NB! Timeout! Normally we set these &quot;blocking&quot; polls to time out
            // when we calculate the next timer to expire, we set that as the
            // timeout to our epoll queue. Then we block the loop while waiting
            // for an event to happen or a timeout to expire.
            self.process_epoll_events();
            self.process_threadpool_events();
            self.run_callbacks();

            // ===== 5. CHECK =====
            // an set immidiate function could be added pretty easily but we 
            // won't do that here

            // ===== 6. CLOSE CALLBACKS ======
            // Release resources, we won't do that here, but this is typically
            // where sockets etc are closed.

            // Let the OS have a time slice of our thread so we don't busy loop
            // this could be dynamically set depending on requirements or load.
            thread::park_timeout(std::time::Duration::from_millis(1));
        }
        print(&quot;FINISHED&quot;);
    }
}
#}</code></pre></pre>
<p>I present the full function here to get a overview since this will be what is
running our runtime. As you see I've made several comments where there are steps
that are performed by Node but which we will skip.</p>
<p>I'll step through each step here, and while I do that I will try to point out
where the real Node runtime differs substantially from ours.</p>
<h2><a class="header" href="#initialization" id="initialization">Initialization</a></h2>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
let rt_ptr: *mut Runtime = self;
unsafe { RUNTIME = rt_ptr as usize };
let mut timers_to_remove = vec![]; 
let mut ticks = 0; // just for us priting out

// First we run our &quot;main&quot; function
f();
#}</code></pre></pre>
<p>The first two lines is just a <code>hack</code> we use in our code to make it &quot;look&quot; more
like javascript. Here we take the pointer to <code>self</code> and set it in the global
variable <code>RUNTIME</code>. We could instead pass our <code>runtime</code> around but that wouldn't
be very ergonomic. Another option would be to use <code>lazy_static</code> crate to initlialize
this field in a safer way, but we'd have to explain what <code>lazy_static</code> do to keep
our promise of minimal &quot;magic&quot;. To be honest, we only set this once, and it's set at
the start of of our eventloop and not touched until it's finished so in this case we
could argue it's safe to do it like this.</p>
<p>The variable <code>timers_to_remove</code> is for us to keep track of the timers we've set.
<code>ticks</code> is only a counter for us to keep track of how many times we've looped
to display.</p>
<p>The first thing we do to kick of the code is invoking <code>f()</code>. <code>f</code> will be the
code we wrote in the <code>javascript</code> function in the last chapter. If this is empty
nothing will happen.</p>
<h2><a class="header" href="#starting-the-event-loop" id="starting-the-event-loop">Starting the event loop</a></h2>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// ===== EVENT LOOP =====
while self.pending_events &gt; 0 {
    ticks += 1;
#}</code></pre></pre>
<p>There are two things to note here:</p>
<p><code>self.pending_events</code> isn't in our runtime struct yet so we need to add that. 
This variable keeps track of how many pending events we have, so that when no 
events are left we exit the loop since our eventloop is finished.</p>
<p>So where does these events come from? In our <code>javascript</code> function in the 
previous chapters you probably noticed that we called functions like 
<code>set_timeout</code> and <code>Fs::read</code>. These functions are defined in the Node runtime 
(as they are in ours), and they don't do much except from regestering events. 
So when one of these events are registered this counter is increased.</p>
<p><code>ticks</code> is just increasing a <code>tick</code> in the counter.</p>
<p>So our Runtime struct looks like this now:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
pub struct Runtime {
    pending_events: usize,
}
#}</code></pre></pre>
<h2><a class="header" href="#1-effectuate-timers" id="1-effectuate-timers">1. Effectuate timers</a></h2>
<p><code>self.effectuate_timers(&amp;mut timers_to_remove);</code></p>
<p>We check here if any timers has expired. I couldn't find a better word for it
than <code>effectuate</code> but it basically mean that if we have timers that are expired
we schedule the callbacks for the expired timers to run at the first call to <code>self.run_callbacks()</code>.</p>
<p>Worth noting here is that timers with a timeout of <code>0</code> will already have timed
out by the time we reach this function.</p>
<h2><a class="header" href="#2-callbacks" id="2-callbacks">2. Callbacks</a></h2>
<p><code>self.run_callbacks();</code></p>
<p>Now we could have ran the callbacks in the timer <code>step</code> but since this is the next
step of our loop we do it here instead.</p>
<blockquote>
<p>This step might seem unnecessary here but in Node it has a function. Some
types of callbacks will be deferred to the <code>next tick</code>, which means that they're
not run immediately, but deferred to this step on the next loop. We won't implement
this functionality here but it's worth noting.</p>
</blockquote>
<h2><a class="header" href="#3-idleprepare" id="3-idleprepare">3. Idle/Prepare</a></h2>
<p>This is a step mostly used by Nodes internals. It's not important for understanding
the big picture here but I included it since it's something you see in Nodes
documentation so you know where we're at in the loop at this point.</p>
<h2><a class="header" href="#4-poll" id="4-poll">4. Poll</a></h2>
<p>This is really where everything happens. I refer to the <code>epoll/kqueue/IOCP</code> 
eventqueue as <code>epoll</code> here just so you know that it's not only <code>epoll</code> we're
waiting for. From now on I will refer to the cross platform event queue as <code>epoll</code>
in the code.</p>
<p>First we check if the OS has reported any events, and if so we schedule the
corresponding callbacks to run.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
self.process_epoll_events();
#}</code></pre></pre>
<p>Next we check if our threadpool has finished any work and if so we schedule their
corresponding callbacks to be run too:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
self.process_threadpool_events();
#}</code></pre></pre>
<p>If our <code>epoll</code> queue or our <code>threadpool</code> registered any callbacks we run them now.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
self.run_callbacks();
#}</code></pre></pre>
<blockquote>
<p>There are some important differences between our implementation and Nodes here.</p>
<p>We only check if any events are registered here and then continue on. This is
suboptimal since if we're wasting cycles by looping when there might be nothing
to do on the next iteration. </p>
<p>Node solves this by calculating the time until the next timeout (in step 1) 
times out. Let's say that the next timer times out in 10 seconds. In node 10 
seconds is then passed as a timeout for the <code>polls</code> in our poll phase so even 
though no event has happened it will wake up again and iterate so it executes 
the next timer when it starts the loop again. As you understand, that means
that the timer will not run at the exact same time as it times out, but it's
potentially much more efficient than what we do here.</p>
</blockquote>
<h2><a class="header" href="#5-check" id="5-check">5. Check</a></h2>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// ===== CHECK =====
// an set immediate function could be added pretty easily but we won't that here
#}</code></pre></pre>
<p>Node implements a check &quot;hook&quot; to the eventloop next. Calls to <code>setImmidiate</code>
execute here. I just include it for for completeness but we won't do anything in this phase.</p>
<h2><a class="header" href="#6-close-callbacks" id="6-close-callbacks">6. Close Callbacks</a></h2>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// ===== CLOSE CALLBACKS ======
// Release resources, we won't do that here, it's just another &quot;hook&quot; for our &quot;extensions&quot;
// to use. We release in every callback instead
#}</code></pre></pre>
<p>I pretty much explain this step in the comments. Typically releasing resources,
like closing sockets, is done here.</p>
<h2><a class="header" href="#shortcuts" id="shortcuts">Shortcuts</a></h2>
<p>I'll mention some obvious shortcuts right here so you are aware of them. There are many &quot;exceptions&quot; that we don't cover in our example. We are focusing on the big picture just so we're on the same page. The <code>process.nextTick</code> function and the <code>setImmediate</code> function are two examples of this. I explained how we did skip the fact that the next timeout will define how long the <code>poll</code> phase will potentially block instead of continue the loop like we do here.</p>
<p>We don't cover the case where a server under heavy load might have too many callbacks to reasonably run in one <code>poll</code> which means that we could starve our I/O resources in the meantime waiting for them to finish, and probably several similar cases that a production
runtime should care about.</p>
<p>As you'll probably notice, implementing a simple version is more than enough work
for us to cover in this book, but hopefully you'll find yourself in pretty good
shape to dig further once we're finished.</p>
<p>If you do want to know more about the Node eventloop I have two talks for you that I find great (and correct) on this subject:</p>
<p>This first one is made held by <a href="https://github.com/piscisaureus">@piscisaureus</a> and is an excellent 15 minute overview:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PNa9OMajw9w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>The second one is slightly longer but is also an excellent talk held by <a href="https://github.com/nebrius">Bryan Hughes</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/zphcsoSJMvM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><h1><a class="header" href="#implementing-the-runtime" id="implementing-the-runtime">Implementing the Runtime</a></h1>
<p>This is a lot to parse and will be a lot to take in right now. But this is also
the heart of our program. Let's go through and explain everything.</p>
<h2><a class="header" href="#1-check-timers" id="1-check-timers">1. Check timers</a></h2>
<p>The first step in the event loop is checking the timers:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// ===== TIMERS =====
self.timers
    .range(..=Instant::now())
    .for_each(|(k, _)| timers_to_remove.push(*k));

while let Some(key) = timers_to_remove.pop() {
    let callback_id = self.timers.remove(&amp;key).unwrap();
    self.next_tick_callbacks.push((callback_id, Js::Undefined));
}

// NOT PART OF LOOP, JUST FOR US TO SEE WHAT TICK IS EXCECUTING
if !self.next_tick_callbacks.is_empty() {
    print(format!(&quot;===== TICK {} =====&quot;, ticks));
}
#}</code></pre></pre>
<p>The first thing to note here is that we check <code>self.timers</code> and to understand the
rest of the syntax we'll have to look what kind of collection this is.</p>
<p>Now I chose a <code>BTreeMap&lt;Instant, usize&gt;</code> for this collection. The reason is that
i want to have many <code>Instant</code>'s chronologically. When I add a timer, I calculate
at what instance it's supposed to be run and I add that to this collection.</p>
<blockquote>
<p>BTrees are a very good data structure when you know that your keys will be ordered.</p>
</blockquote>
<p>Choosing a <code>BTreeMap</code> here allows me to get a range <code>range(..=Instant::noew())</code>
which is from the start of the map, up until or equal to the instant NOW.</p>
<p>Now I take every key in this range and add it to <code>timers_to_remove</code>, and the reason
for this is that I found no good way to both get a range and remove the key's in one
operation without allocating a small buffer every time. You can iterate over the range
but due to the ownership rules you can't remove them at the same time, and we want to
remove the timers, we've run.</p>
<p>The eventloop will run repeatedly so avoiding any allocations inside the loop is smart. There is no need to have this overhead.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
while let Some(key) = timers_to_remove.pop() {
    let callback_id = self.timers.remove(&amp;key).unwrap();
    self.next_tick_callbacks.push((callback_id, Js::Undefined));
}
#}</code></pre></pre>
<p>The next step is to take every timer that has expired, remove the timer from our <code>self.timers</code> collection and get their <code>callback_id</code>.</p>
<p>As I explained in the previous chapter, this is an unique Id for this callback. What's
important here is that we don't run the callback <strong>immediately</strong>. Node actually registers callbacks to be run on the next <code>tick</code>. An exception is the timers since they either have timed out or is a timer with a timeout of <code>0</code>. In this case a timer will not wait for the next tick if it has timed out, or in the case if it has a timeout of <code>0</code> they will be invoked immediately as you'll see next.</p>
<p>Anyway, for now we add the callback id's to <code>self.next_tick_callbacks</code>.</p>
<p>Before we go on. Let's update our <code>Runtime</code> struct to reflect what we've seen:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
pub struct Runtime {
    pending_events: usize,
    next_tick_callbacks: Vec&lt;(usize, Js)&gt;,
    timers: BTreeMap&lt;Instant, usize&gt;,
}
#}</code></pre></pre>
<h2><a class="header" href="#2-process-callbacks" id="2-process-callbacks">2. Process callbacks</a></h2>
<p>The next step is to handle any callbacks we've scheduled to run.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// ===== CALLBACKS =====
while let Some((callback_id, data)) = self.next_tick_callbacks.pop() {
    let cb = self.callback_queue.remove(&amp;callback_id).unwrap();
    cb(data);
    self.pending_events -= 1;
}
#}</code></pre></pre>
<blockquote>
<p>Shortcut. Not all of Nodes callbacks are processed here. Some callbacks is called
directly in the <code>poll</code> phase we'll introduce below. It's not difficult to implement
but it adds unneccecary complexity to our example so we schedula all callbacks to be
run in this step of the process. As long as you know this is an oversimplification
you're going to be alright :)</p>
</blockquote>
<p>Here we <code>pop</code> off all callbacks that are scheduled to run. As you see from our last update on the <code>Runtime</code> struct. <code>next_tick_callbacks</code> is an array of callback_id and an argument type of <code>Js</code>.</p>
<p>So when we've got a <code>callback_id</code> we find the corresponding callback we have stored in <code>self.callback_queue</code> and remove the entry. What we get in return is a callback of type
<code>Box&lt;dyn FnOnce(Js)&gt;</code>. We're going to explain this type more later but it's basically a closure stored on the heap that takes one argument of type <code>Js</code>.</p>
<p><code>cb(data)</code> runs the code in this closure. After it's done it's time to decrease our counter of pending events: <code>self.pending_events -= 1;</code>.</p>
<blockquote>
<p>Now, this step is important. As you might understand, any long running code in this callback is going to block our <code>eventloop</code>, preventing it from progressing. So no new callbacks are handleded and no new events are registered. This is why it's bad to write code that blocks the eventloop.</p>
</blockquote>
<p>Let's update our Runtime struct again:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
pub struct Runtime {
    callback_queue: HashMap&lt;usize, Box&lt;dyn FnOnce(Js)&gt;&gt;,
    pending_events: usize,
    next_tick_callbacks: Vec&lt;(usize, Js)&gt;,
    timers: BTreeMap&lt;Instant, usize&gt;,
}
#}</code></pre></pre>
<h2><a class="header" href="#3-idleprepare-1" id="3-idleprepare-1">3. Idle/prepare</a></h2>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
// ===== IDLE/PREPARE =====
// we won't use this
#}</code></pre></pre>
<p>The Idle/Prepare step is reportedly used internally by Node. They're not so interesting for us in understanding Nodes eventloop so we skip this step.</p>
<h2><a class="header" href="#4-poll-1" id="4-poll-1">4. Poll</a></h2>
<p>The next phase is to check if any events are ready from either our threadpool or our eventqueue.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
 // ===== POLL =====
// First poll any epoll/kqueue/IOCP
while let Ok(event_id) = self.epoll_reciever.try_recv() {
    let id = self
        .epoll_event_cb_map
        .get(&amp;(event_id as i64))
        .expect(&quot;Event not in event map.&quot;);
    let callback_id = *id;
    self.epoll_event_cb_map.remove(&amp;(event_id as i64));

    self.next_tick_callbacks.push((callback_id, Js::Undefined));
    self.epoll_pending -= 1;
}

// then check if there is any results from the threadpool
while let Ok((thread_id, callback_id, data))self.threadp_reciever.try_recv() {
    self.next_tick_callbacks.push((callback_id, data));
    self.available.push(thread_id);
}
#}</code></pre></pre>
<p>There is a lot going on here so let's step through it:</p>
<p>First we check our <code>epoll/kqueue/IOCP</code> event queue and see if anything events are ready.</p>
<p>The first thing we do is to check if there are any incoming messages on our channel
<code>self.epoll_reciever.try_recv()</code>, as you'll see when we define this in our <code>Runtime</code> I chose to implement this using Rusts channels which is a good fit for this. No need to make it more complicated than it is.</p>
<p>We're choosing to keep track on how many epoll events we're waiting for in
<code>self.epoll_pending</code>, so once an event is ready we'll decrement this to reflect
that we have one less event in the I/O eventloop.</p>
<p>If any events has occured we get an <code>event_id</code>. Since <code>event_id</code>'s can potentially overlap with Id's we have given previous callbacks we use a map where we give this <code>event</code> an unique <code>callback_id</code> that ties the event to the callback we have registered.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
self.epoll_event_cb_map
        .get(&amp;(event_id as i64))
        .expect(&quot;Event not in event map.&quot;);
#}</code></pre></pre>
<p>Retrieves the <code>callback_id</code> we stored with this event which we then remove from the map
<code>self.epoll_event_cb_map.remove(&amp;(event_id as i64))</code> so we don't store it indefinitely.</p>
<p>The next two steps is the same as you saw used in the timer segment. We schedule the callback to get run on the next tick.</p>
<blockquote>
<p>One thing to note is that we pass in Js::Undefined here too even though the callback we registered is expecting data. The reason for this is that we wrap the callback to accommodate for the difference between <code>epoll/kqueue</code> and <code>IOCP</code>. In the case of <code>epoll/kqueue</code> we read the data into a buffer we pass in before we call the callback, and in the case of <code>IOCP</code> the data is already filled for us.</p>
</blockquote>
<p>The next thing to check is our threadpool. As you see here we also use <code>Channel</code> here to communicate.</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
while let Ok((thread_id, callback_id, data))self.threadp_reciever.try_recv() {
    self.next_tick_callbacks.push((callback_id, data));
    self.available.push(thread_id);
}
#}</code></pre></pre>
<p>We get some more data here. Namely a <code>thread_id</code>, <code>callback_id</code> and <code>data</code>. Now we need the <code>thread_id</code> to mark this thread as <code>available</code> so it can be used on subsequent calls to the threadpool. The <code>callback_id</code> we need in all cases to know what callback to invoke. One difference here is that the thread also holds the data we want to pass in to our callback so we also get that an pass that in to our <code>next_tick_callbacks</code> so it's available to our callback on the next tick.</p>
<p>Now we introduced a lot of new members of our <code>Runtime</code> struct here and it's almost
finished:</p>
<pre><pre class="playpen"><code class="language-rust">
# #![allow(unused_variables)]
#fn main() {
pub struct Runtime {
    callback_queue: HashMap&lt;usize, Box&lt;dyn FnOnce(Js)&gt;&gt;,
    next_tick_callbacks: Vec&lt;(usize, Js)&gt;,
    identity_token: usize,
    pending_events: usize,
    threadp_reciever: Receiver&lt;(usize, usize, Js)&gt;,
    epoll_reciever: Receiver&lt;usize&gt;,
    epoll_pending: usize,
    epoll_event_cb_map: HashMap&lt;i64, usize&gt;,
    timers: BTreeMap&lt;Instant, usize&gt;,
    epoll_registrator: minimio::Registrator,
}
#}</code></pre></pre>
<h2><a class="header" href="#moving-on" id="moving-on">Moving on</a></h2>
<p>Now we've already gotten really far by explaining how our eventloop works already
in the first chapter. Now we just need to set up the infrastructure for this
loop to work.</p>
<h1><a class="header" href="#the-threadpool" id="the-threadpool">The threadpool</a></h1>
<h1><a class="header" href="#the-io-eventqueue" id="the-io-eventqueue">The I/O eventqueue</a></h1>
<h1><a class="header" href="#shortcuts-and-improvements" id="shortcuts-and-improvements">Shortcuts and improvements</a></h1>
<h2><a class="header" href="#shortcuts-1" id="shortcuts-1">Shortcuts</a></h2>
<p>This is the event loop. There are several things we could do here to make it a better implementation. One is to set a max backlog of callbacks to execute in a single tick, so we don't starve the threadpool or file handlers. Another is to dynamically decide if/and how long the thread could be allowed to be parked for example by looking at the backlog of events, and if there is any backlog disable it. Some of our Vec's will only grow, and not resize, so if we have a period of very high load,the memory will stay higher than we need until a restart. This could be dealt with or a differentdata structure could be used.</p>
<h1><a class="header" href="#final-code" id="final-code">Final code</a></h1>
<h1><a class="header" href="#conclusion" id="conclusion">Conclusion</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
